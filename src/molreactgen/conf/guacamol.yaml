# data_args
# dataset_name:
# dataset_config_name:
dataset_dir: ../../../data/prep/guacamol/csv/
# The split percentages are only relevant if there is no file for each split
# validation_split_percentage: 10
# test_split_percentage: 10
# max_train_samples: 1000
# max_val_samples: 100
preprocessing_num_workers: 4

# Tokenizer
# One of char, wordlevel, bpe, sentencepiece_bpe, sentencepiece_unigram
tokenizer_algorithm: wordlevel
vocab_min_frequency: 2
vocab_size: 0


# model_args
# model_name_or_path: name | path
model_type: gpt2

# see https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/gpt2#transformers.GPT2Config
# n_embd must be divisible by n_head
config_overrides: "
n_positions=896,
n_embd=132,
n_layer=12,
n_head=12
"
# activation_function=gelu_new,
# reorder_and_upcast_attn=true
# scale_attn_by_inverse_layer_idx=true

use_auth_token: true


# training_args
output_dir: "./checkpoints/"
overwrite_output_dir: false
do_train: true
do_eval: true
do_predict: true
evaluation_strategy: epoch
per_device_train_batch_size: 128
per_device_eval_batch_size: 128
gradient_accumulation_steps: 4
learning_rate: 0.0005  # cannot use 5e-4 here; is interpreted as a string
weight_decay: 0.1
adam_beta1: 0.9
adam_beta2: 0.95
max_grad_norm: 1.0  # = default
num_train_epochs: 50
lr_scheduler_type: cosine_with_restarts
warmup_ratio: 0.1  # what's a "good" value here?
log_level: warning
logging_strategy: steps
logging_steps: 100
save_strategy: epoch
save_total_limit: 5
seed: 42
fp16: false  # might change to true, gpu only
dataloader_num_workers: 4
# past_index: 2  # might use for TransformerXL or XLNet
# run_name: MolReactGen
disable_tqdm: false
load_best_model_at_end: true
# label_smoothing_factor: 0.1  # TODO might play with, not implemented yet
group_by_length: true
report_to: wandb
push_to_hub: true
hub_model_id: MolReactGen
hub_strategy: end
hub_private_repo: true
# resume_from_checkpoint: true | directory_path
