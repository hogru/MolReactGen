# uspto50k.yaml

# data_args
# dataset_name:
# dataset_config_name:
dataset_dir: ../../../data/prep/uspto-50k/csv/with_mapping
# The split percentages are only relevant if there is no file for each split
# validation_split_percentage: 10
# test_split_percentage: 10
# max_train_samples: 1000
# max_eval_samples: 100
preprocessing_num_workers: 4

# Tokenizer
# One of char, wordlevel, bpe, sentencepiece_bpe, sentencepiece_unigram
pre_tokenizer: atom
algorithm: wordlevel
vocab_min_frequency: 1
vocab_size: 0


# model_args
# model_name_or_path: name | path
model_type: gpt2

# see https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/gpt2#transformers.GPT2Config
# n_embd must be divisible by n_head
# use multiples of 8 for GPU
config_overrides: "
n_positions=896,
n_embd=132,
n_layer=12,
n_head=12
"
use_auth_token: true


# training_args
output_dir: "../../checkpoints/"
overwrite_output_dir: false
do_train: true
do_eval: true
do_predict: true
evaluation_strategy: epoch
per_device_train_batch_size: 16
per_device_eval_batch_size: 32
gradient_accumulation_steps: 8
learning_rate: 0.0005  # cannot use 5e-4 here; is interpreted as a string
weight_decay: 0.1
adam_beta1: 0.9
adam_beta2: 0.95
max_grad_norm: 1.0  # = default
num_train_epochs: 100
lr_scheduler_type: cosine
warmup_ratio: 0.1
log_level: warning
logging_strategy: steps
logging_steps: 100
save_strategy: epoch
save_total_limit: 5
seed: 42
fp16: false  # TODO might change to true
dataloader_num_workers: 4
# past_index: 2  # might use for TransformerXL or XLNet
run_name: MolReactGen  # TODO might determine during runtime
disable_tqdm: false
load_best_model_at_end: true
# label_smoothing_factor: 0.1  # TODO might play with, not implemented yet
group_by_length: true
report_to: wandb
push_to_hub: false
hub_model_id: MolReactGen
hub_strategy: end
hub_private_repo: true
# resume_from_checkpoint: true | directory_path
