# debug.yaml

# data_args
# dataset_name:
# dataset_config_name:
dataset_dir: ../../data/prep/debug/csv/
# The split percentages are only relevant if there is no file for each split
validation_split_percentage: 10
test_split_percentage: 10
# max_train_samples: 1000
# max_val_samples: 100
preprocessing_num_workers: 4

# Tokenizer
# One of char, wordlevel, bpe, sentencepiece_bpe, sentencepiece_unigram
pre_tokenizer: char
algorithm: bpe
vocab_min_frequency: 1
vocab_size: 0
map_tokenizers: true


# model_args
model_name_or_path: gpt2
model_type: gpt2

# see https://huggingface.co/docs/transformers/v4.24.0/en/model_doc/gpt2#transformers.GPT2Config
# n_embd must be divisible by n_head
# use multiples of 8 for GPU
# config_overrides: "
# n_positions=128,
# n_embd=144,
# n_layer=4,
# n_head=4
# "
use_auth_token: true


# training_args
output_dir: "../../checkpoints/"
overwrite_output_dir: false
do_train: true
do_eval: true
do_predict: true
evaluation_strategy: epoch
per_device_train_batch_size: 32
per_device_eval_batch_size: 16
gradient_accumulation_steps: 8
learning_rate: 0.0005  # cannot use 5e-4 here; is interpreted as a string
weight_decay: 0.1
adam_beta1: 0.9
adam_beta2: 0.95
max_grad_norm: 1.0  # = default
num_train_epochs: 2

# the options cosine_with_restarts falls back to "cosine" due to this bug:
# https://github.com/huggingface/transformers/issues/20552
lr_scheduler_type: cosine
warmup_ratio: 0.1
log_level: info
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_total_limit: 5
seed: 42
fp16: false  # might change to true, gpu only
dataloader_num_workers: 4
# past_index: 2  # might use for TransformerXL or XLNet
# run_name: MolReactGen
disable_tqdm: false
load_best_model_at_end: true
# label_smoothing_factor: 0.1  # TODO might play with, not implemented yet
group_by_length: true
report_to: wandb
push_to_hub: false
hub_model_id: MolReactGen
hub_strategy: end
hub_private_repo: true
# resume_from_checkpoint: true | directory_path

# additional_args
early_stopping_patience: 5
unique_output_subdir: true
