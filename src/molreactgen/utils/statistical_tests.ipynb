{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# General Setup & Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a6405014065f5e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Final, Iterable, Optional, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, ttest_1samp, ttest_ind  # t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.558633992Z",
     "start_time": "2023-11-24T11:24:00.155353695Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "ALTERNATIVES = (\"two-sided\", \"less\", \"greater\")\n",
    "ALPHA = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.562910980Z",
     "start_time": "2023-11-24T11:24:00.484368123Z"
    }
   },
   "id": "444ab0135c733968"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    \"\"\"Simple class to hold an array of values, their mean and standard deviation.\"\"\"\n",
    "\n",
    "    values: Sequence[float]\n",
    "    array: np.ndarray = None\n",
    "    ddof: int = 1\n",
    "    mean: float = None\n",
    "    std: float = None\n",
    "    variance: float = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.array = np.array(self.values)\n",
    "        self.mean, self.std, self.var = self.get_basic_stats()\n",
    "\n",
    "    def get_basic_stats(self) -> tuple[float, float, float]:\n",
    "        mean = np.mean(self.array)\n",
    "        std = np.std(self.array, ddof=self.ddof)\n",
    "        var = np.var(self.array, ddof=self.ddof)\n",
    "        return mean, std, var\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Sample({self.values})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.array}\"\n",
    "\n",
    "\n",
    "MetricValue = Union[float, Sample]\n",
    "Metric = dict[str, MetricValue]\n",
    "TokenizerResults = dict[str, Metric]\n",
    "\n",
    "\n",
    "def build_tokenizer_results(\n",
    "    tokenizer_names: Sequence[str],\n",
    "    **kwargs,\n",
    ") -> TokenizerResults:\n",
    "    \"\"\"Helper function to build a data structure from raw experiment data.\"\"\"\n",
    "    results: TokenizerResults = {}\n",
    "\n",
    "    for metric_name, metric in kwargs.items():\n",
    "        assert len(metric) == len(\n",
    "            tokenizer_names\n",
    "        ), f\"Incorrect number of metrics for {metric_name}\"\n",
    "        assert isinstance(metric, dict), f\"Incorrect metric {metric} in {metric_name}\"\n",
    "        for tokenizer_name, metric_value in metric.items():\n",
    "            assert (\n",
    "                tokenizer_name in tokenizer_names\n",
    "            ), f\"Incorrect tokenizer {tokenizer_name} in {metric_name}\"\n",
    "            assert isinstance(metric_value, float) or isinstance(\n",
    "                metric_value, Sample\n",
    "            ), f\"Incorrect metric value {metric_value} in {metric_name}\"\n",
    "\n",
    "    for tokenizer_name in tokenizer_names:\n",
    "        results[tokenizer_name] = {}\n",
    "\n",
    "    for metric_name, metric in kwargs.items():\n",
    "        for tokenizer_name, metric_value in metric.items():\n",
    "            results[tokenizer_name][metric_name] = metric_value\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_mean_value(metric_value: MetricValue) -> float:\n",
    "    \"\"\"Helper function to return either the mean of a Sample or the value itself if float.\"\"\"\n",
    "    if isinstance(metric_value, float):\n",
    "        return metric_value\n",
    "    elif isinstance(metric_value, Sample) or type(metric_value).__name__ == \"Sample\":\n",
    "        return metric_value.mean\n",
    "    else:\n",
    "        # pass\n",
    "        # Commented out the TypeError because for unknown reasons a Sample was not considered a sample (was of type \"__main__.Sample\")\n",
    "        raise TypeError(\n",
    "            f\"metric_value must be either float or Sample, but has type {type(metric_value).__name__}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_std_value(metric_value: MetricValue) -> Optional[float]:\n",
    "    \"\"\"Helper function to return either the mean of a Sample or the value itself if float.\"\"\"\n",
    "    if isinstance(metric_value, float):\n",
    "        return None\n",
    "    elif isinstance(metric_value, Sample) or type(metric_value).__name__ == \"Sample\":\n",
    "        return metric_value.std\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"metric_value must be either float or Sample, but has type {type(metric_value).__name__}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def print_basic_stats(\n",
    "    results: TokenizerResults, sort_by: str = \"fcd\", reverse: bool = False\n",
    "):\n",
    "    \"\"\"Helper function to print sorted mean (and standard deviation, if available) of tokenizer results.\"\"\"\n",
    "    reverse_string = \"descending\" if reverse else \"ascending\"\n",
    "    print(\n",
    "        f\"Printing basic stats for {len(results)} tokenizers sorted by {sort_by.upper()} in {reverse_string.upper()} order...\"\n",
    "    )\n",
    "    # for tokenizer_name in sorted(\n",
    "    #         results, key=lambda x: get_mean_value(results[x][sort_by]), reverse=reverse\n",
    "    # ):\n",
    "    for tokenizer_name, metrics in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1][sort_by]), reverse=reverse\n",
    "    ):\n",
    "        print(f\"\\n*** {tokenizer_name.upper()} ***\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, float):\n",
    "                print(f\"Metric: {metric_name:15s} Single value: {metric_value:.3f}\")\n",
    "            elif isinstance(metric_value, Sample):\n",
    "                print(\n",
    "                    f\"Metric: {metric_name:15s} Mean:         {metric_value.mean:.3f}   Std.dev. {metric_value.std:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                raise TypeError(\"metric must be a dict[str, dict[float|Sample]]\")\n",
    "\n",
    "\n",
    "def get_best_tokenizer(\n",
    "    results: TokenizerResults,\n",
    "    sort_by: str = \"fcd\",\n",
    "    reverse: bool = False,\n",
    "    with_sample: bool = True,\n",
    ") -> tuple[str, MetricValue]:\n",
    "    \"\"\"Helper function to determine the best performing tokenizer.\"\"\"\n",
    "    best_tokenizers = sorted(\n",
    "        results, key=lambda x: get_mean_value(results[x][sort_by]), reverse=reverse\n",
    "    )\n",
    "    if not with_sample:\n",
    "        best_tokenizer = best_tokenizers[-1]\n",
    "    else:\n",
    "        tokenizers_with_samples = get_tokenizers_with_sample(results, criterion=sort_by)\n",
    "        i = -1\n",
    "        while best_tokenizer := best_tokenizers[i]:\n",
    "            if best_tokenizer in tokenizers_with_samples:\n",
    "                break\n",
    "            else:\n",
    "                i -= 1\n",
    "\n",
    "    metric_value = results[best_tokenizer][sort_by]\n",
    "    return best_tokenizer, metric_value\n",
    "\n",
    "\n",
    "def get_tokenizers_with_sample(\n",
    "    results: TokenizerResults, criterion: str = \"fcd\"\n",
    ") -> list[str]:\n",
    "    \"\"\"Helper function to determine tokenizers which have a Sample for a criterion (and not a float).\"\"\"\n",
    "    result = []\n",
    "    tokenizers = results.keys()\n",
    "    for tokenizer in tokenizers:\n",
    "        metric_value = results[tokenizer][criterion]\n",
    "        if not isinstance(metric_value, float):  # (metric_value, Sample):\n",
    "            result.append(tokenizer)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_fcd_guacamol(fcd: Metric) -> Metric:\n",
    "    \"\"\"Helper function to calculate the FCD in 'GuacaMol style'\"\"\"\n",
    "    fcd_guacamol: Metric = {}\n",
    "    for tokenizer_name, fcd_value in fcd.items():\n",
    "        if isinstance(fcd_value, float):\n",
    "            fcd_guacamol[tokenizer_name] = np.exp(-0.2 * fcd_value)\n",
    "        elif isinstance(fcd_value, Sample):\n",
    "            values = [np.exp(-0.2 * fcd) for fcd in fcd_value.values]\n",
    "            fcd_guacamol[tokenizer_name] = Sample(values)\n",
    "        else:\n",
    "            raise TypeError(\"Wrong type for fcd\")\n",
    "    return fcd_guacamol\n",
    "\n",
    "\n",
    "def print_comparison(\n",
    "    tokenizer_name: str,\n",
    "    # tokenizer_sample: Sample,\n",
    "    results: TokenizerResults,\n",
    "    criterion: str = \"fcd\",\n",
    "    alternative: str = \"two_sided\",\n",
    "    reverse: bool = False,\n",
    "):\n",
    "    \"\"\"Compares tokenizer results and does the statistical tests.\"\"\"\n",
    "    reverse_string = \"descending\" if reverse else \"ascending\"\n",
    "    print(\"\\n*** Perform statistical tests ***\")\n",
    "    print(\n",
    "        f\"Compare metric {criterion.upper()} with other tokenizers (in {reverse_string} order)\"\n",
    "    )\n",
    "    print(f\"Alternative: {alternative.upper()}\")\n",
    "    tokenizer_sample = results[tokenizer_name][criterion]\n",
    "    print(\n",
    "        f\"Best tokenizer: {tokenizer_name} with metric mean {tokenizer_sample.mean:.3f} and std.dev. {tokenizer_sample.std:.3f}\"\n",
    "    )\n",
    "    for tokenizer, metrics in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1][criterion]), reverse=reverse\n",
    "    ):\n",
    "        metric = metrics[criterion]\n",
    "        mean = get_mean_value(metric)\n",
    "        std = get_std_value(metric)\n",
    "        if std is None:\n",
    "            print(f\"\\n{tokenizer:20s} with metric single value {mean:.3f} \")\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\n{tokenizer:20s} with metric mean {mean:.3f} \"\n",
    "                f\"and std.dev. {std:.3f}\"\n",
    "            )\n",
    "        t_stat, p_val = t_test(tokenizer_sample, metric, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:6.3f}, \"\n",
    "            f\"p-value: {p_val:6.3f}, \"\n",
    "            f\"Reject H0 (p<{ALPHA}): {p_val < ALPHA}\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.563072970Z",
     "start_time": "2023-11-24T11:24:00.484541245Z"
    }
   },
   "id": "2e1f4a4d72352cdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Statistical Tests Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be786b18c8ea5098"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def is_normally_distributed(sample: Sample, alpha: float = ALPHA):\n",
    "    \"\"\"Perform Shapiro Test for normal distribution.\"\"\"\n",
    "    _, p_value = shapiro(sample.array)\n",
    "    return p_value > alpha"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.563199875Z",
     "start_time": "2023-11-24T11:24:00.484671491Z"
    }
   },
   "id": "c0d4cdc77bbde306"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def one_sample_t_test(\n",
    "    sample: Sample, population_mean: float, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform one sample Student's t-test (or Wilcoxon signed rank test).\"\"\"\n",
    "    # n = len(sample)\n",
    "    # df = n - 1\n",
    "    # t_statistic = (sample.mean - population_mean) / (sample.std / np.sqrt(n))\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    test_statistic, p_value = ttest_1samp(\n",
    "        sample.array, population_mean, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.563320329Z",
     "start_time": "2023-11-24T11:24:00.484798012Z"
    }
   },
   "id": "d2ce292f1d0d6014"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def two_sample_t_test(\n",
    "    sample: Sample, baseline: Sample, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Perform one sample Student's t-test (or Welch's t-test).\"\"\"\n",
    "    # n1 = len(sample)\n",
    "    # n2 = len(baseline)\n",
    "    if sample.std > 2.0 * baseline.std or baseline.std > 2.0 * sample.std:\n",
    "        print(\"Warning: standard deviations differ by more than a factor of 2\")\n",
    "        print(\"Therefore, we do a Welch's t-test instead of a Student's t-test!\")\n",
    "        equal_var = False\n",
    "    else:\n",
    "        equal_var = True\n",
    "\n",
    "    # pooled_std = np.sqrt(((n1 - 1) * sample.std ** 2 + (n2 - 1) * baseline.std ** 2) / (n1 + n2 - 2))\n",
    "    # t_statistic = (sample.mean - baseline.mean) / (pooled_std * np.sqrt(1 / n1 + 1 / n2))\n",
    "    # df = n1 + n2 - 2\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    if not is_normally_distributed(baseline):\n",
    "        print(\"Warning: baseline is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "\n",
    "    test_statistic, p_value = ttest_ind(\n",
    "        sample.array, baseline.array, equal_var=equal_var, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.565522837Z",
     "start_time": "2023-11-24T11:24:00.491894562Z"
    }
   },
   "id": "1eaaa267aedff3ba"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def interpret_p_value(\n",
    "    p_value: float,\n",
    "    # alternative: str,\n",
    "    alpha: float = ALPHA,\n",
    ") -> str:\n",
    "    \"\"\"Helper function to determine if we can or can not reject the null hypothesis.\"\"\"\n",
    "    if p_value < alpha:\n",
    "        return \"Reject H0\"\n",
    "    else:\n",
    "        return \"Cannot reject H0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.596721245Z",
     "start_time": "2023-11-24T11:24:00.507069097Z"
    }
   },
   "id": "46ab88ffb3d0f902"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def t_test(\n",
    "    sample: Sample, comparison: Union[float, Sample], alternative: str = \"two_sided\"\n",
    "):\n",
    "    \"\"\"Helper function to select either a one-sample or two-sample t-test, depending on whether we have a Sample or a float.\"\"\"\n",
    "    if isinstance(comparison, float):\n",
    "        t_statistic, p_value = one_sample_t_test(sample, comparison, alternative)\n",
    "    elif isinstance(comparison, Sample) or type(comparison).__name__ == \"Sample\":\n",
    "        t_statistic, p_value = two_sample_t_test(sample, comparison, alternative)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"comparison must be a float or a Sample but is {type(comparison).__name__}\"\n",
    "        )\n",
    "\n",
    "    return t_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.597063227Z",
     "start_time": "2023-11-24T11:24:00.522316136Z"
    }
   },
   "id": "91c8db71576e9a1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279ffc159089ec11"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test example\n",
      "Sample is normally distributed: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapiro-Wilk test example\")\n",
    "sample = Sample([2.5, 3.1, 2.8, 3.4, 2.9, 3.0, 3.3, 2.6, 3.2, 3.1])\n",
    "result = is_normally_distributed(sample)\n",
    "print(f\"Sample is normally distributed: {result}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.597494203Z",
     "start_time": "2023-11-24T11:24:00.532801189Z"
    }
   },
   "id": "fbf7881d94b693a0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample t-test example\n",
      "Sample: [10. 11. 12. 13. 14.]\n",
      "Population mean: 14.0\n",
      "Alternative: two-sided, t-statistic: -2.828, p-value: 0.047, Reject H0 (p<0.05): True\n",
      "Alternative: less     , t-statistic: -2.828, p-value: 0.024, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.828, p-value: 0.976, Reject H0 (p<0.05): False\n",
      "\n",
      "Two sample t-test example\n",
      "Sample: [10. 11. 12.]\n",
      "Baseline: [12. 13. 14.]\n",
      "Alternative: two-sided, t-statistic: -2.449, p-value: 0.070, Reject H0 (p<0.05): False\n",
      "Alternative: less     , t-statistic: -2.449, p-value: 0.035, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.449, p-value: 0.965, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"One sample t-test example\")\n",
    "sample = Sample([10.0, 11.0, 12.0, 13.0, 14.0])\n",
    "population_mean = 14.0\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Population mean: {population_mean}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = one_sample_t_test(sample, population_mean, alternative)\n",
    "    t_stat, p_val = t_test(sample, population_mean, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTwo sample t-test example\")\n",
    "sample = Sample(np.array([10.0, 11.0, 12.0]))\n",
    "baseline = Sample(np.array([12.0, 13.0, 14.0]))\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Baseline: {baseline}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = two_sample_t_test(sample, baseline, alternative)\n",
    "    t_stat, p_val = t_test(sample, baseline, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.682965994Z",
     "start_time": "2023-11-24T11:24:00.549987700Z"
    }
   },
   "id": "84baad5a7392d8a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment Results\n",
    "\n",
    "## Guacamol/SMILES\n",
    "\n",
    "### Description\n",
    "\n",
    "#### The initial \"best\" tokenizer\n",
    "\n",
    "This is based on a single run for each tokenizer.\n",
    "A single run consists of:\n",
    "\n",
    "- a training with fixed seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric\n",
    "\n",
    "Based on this setup, char_wordpiece_176 came up as the initial \"best\" tokenizer. However, there are a number of additional \"candidate\" tokenizers which did not show a statistical significant difference, i.e. we could not reject the null hypothesis. We than ran 5 runs with each of those \"candidate\" tokenizers to assess their mean performance and standard deviation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb8d2d2b2f7ebb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical tests for the \"canidate\" tokenizers\n",
    "\n",
    "This is based on 5 runs for the \"candidate\" tokenizers.\n",
    "The 5 runs are:\n",
    "\n",
    "- a training with random seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric\n",
    "\n",
    "At this point we have 5 FCD values for each of the \"candidate\" tokenizers. We define the final \"best\" tokenizer as the winner and perform statistical tests again. This results in a list of tokenizers which show no statistically significant difference in performance and remaining tokenizers for which we can reject the null hypothesis.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c7f589001ae7dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizer Summary\n",
    "\n",
    "- The initial \"best\" tokenizer: char_wordpiece_176\n",
    "- The \"candidate\" tokenizers are: char_bpe_88, char_bpe_176, char_unigram_88\n",
    "- The final \"best\" tokenizer is: char_wordpiece_176"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ed115b13034612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results\n",
    "\n",
    "#### Data Entry"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "894b82524433e17b"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing basic stats for 11 tokenizers sorted by FCD in DESCENDING order...\n",
      "\n",
      "*** CHAR_WORDPIECE_88 ***\n",
      "Metric: validity        Single value: 0.986\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.936\n",
      "Metric: fcd             Single value: 0.243\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** SMARTS_WORDLEVEL_106 ***\n",
      "Metric: validity        Single value: 0.980\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.941\n",
      "Metric: fcd             Single value: 0.241\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** ATOM_WORDLEVEL_50 ***\n",
      "Metric: validity        Single value: 0.981\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.943\n",
      "Metric: fcd             Single value: 0.239\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** CHAR_BPE_44 ***\n",
      "Metric: validity        Single value: 0.983\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.945\n",
      "Metric: fcd             Single value: 0.236\n",
      "Metric: fcd_guacamol    Single value: 0.954\n",
      "\n",
      "*** CHAR_UNIGRAM_176 ***\n",
      "Metric: validity        Single value: 0.976\n",
      "Metric: uniqueness      Single value: 1.000\n",
      "Metric: novelty         Single value: 0.937\n",
      "Metric: fcd             Single value: 0.232\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_UNIGRAM_44 ***\n",
      "Metric: validity        Single value: 0.983\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.938\n",
      "Metric: fcd             Single value: 0.230\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_WORDLEVEL_38 ***\n",
      "Metric: validity        Single value: 0.982\n",
      "Metric: uniqueness      Single value: 1.000\n",
      "Metric: novelty         Single value: 0.947\n",
      "Metric: fcd             Single value: 0.226\n",
      "Metric: fcd_guacamol    Single value: 0.956\n",
      "\n",
      "*** CHAR_BPE_88 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.001\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.937   Std.dev. 0.003\n",
      "Metric: fcd             Mean:         0.223   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.956   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_UNIGRAM_88 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.002\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.938   Std.dev. 0.002\n",
      "Metric: fcd             Mean:         0.222   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.956   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_BPE_176 ***\n",
      "Metric: validity        Mean:         0.975   Std.dev. 0.002\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.933   Std.dev. 0.003\n",
      "Metric: fcd             Mean:         0.220   Std.dev. 0.004\n",
      "Metric: fcd_guacamol    Mean:         0.957   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_WORDPIECE_176 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.001\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.935   Std.dev. 0.002\n",
      "Metric: fcd             Mean:         0.219   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.957   Std.dev. 0.001\n"
     ]
    }
   ],
   "source": [
    "# Gather data from experiments and get basic stats\n",
    "\n",
    "GUACAMOL_TOKENIZERS: Final[set[str]] = {\n",
    "    \"char_wordlevel_38\",\n",
    "    \"char_bpe_44\",\n",
    "    \"char_bpe_88\",\n",
    "    \"char_bpe_176\",\n",
    "    \"char_wordpiece_88\",\n",
    "    \"char_wordpiece_176\",\n",
    "    \"char_unigram_44\",\n",
    "    \"char_unigram_88\",\n",
    "    \"char_unigram_176\",\n",
    "    \"atom_wordlevel_50\",\n",
    "    \"smarts_wordlevel_106\",\n",
    "}\n",
    "validity: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9822,\n",
    "    \"char_bpe_44\": 0.9826,\n",
    "    \"char_bpe_88\": Sample([0.9768, 0.9781, 0.9763, 0.9746, 0.9763]),\n",
    "    \"char_bpe_176\": Sample([0.9769, 0.9715, 0.9753, 0.9753, 0.9754]),\n",
    "    \"char_wordpiece_88\": 0.986,\n",
    "    \"char_wordpiece_176\": Sample([0.9779, 0.9769, 0.9759, 0.9757, 0.9753]),\n",
    "    \"char_unigram_44\": 0.9833,\n",
    "    \"char_unigram_88\": Sample([0.9759, 0.9767, 0.9738, 0.9781, 0.9774]),\n",
    "    \"char_unigram_176\": 0.9756,\n",
    "    \"atom_wordlevel_50\": 0.9805,\n",
    "    \"smarts_wordlevel_106\": 0.9796,\n",
    "}\n",
    "uniqueness: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9997963754836082,\n",
    "    \"char_bpe_44\": 0.9990840626908203,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.9987714987714987,\n",
    "            0.999386565790819,\n",
    "            0.9998975724674792,\n",
    "            0.9995895752103428,\n",
    "            0.9990781522073133,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.9992834476404955,\n",
    "            0.9997941327843541,\n",
    "            0.9988721419050548,\n",
    "            0.9989746744591408,\n",
    "            0.9992823457043264,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.9989858012170385,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.9986706207178648,\n",
    "            0.9992834476404955,\n",
    "            0.9995901219387232,\n",
    "            0.9994875474018653,\n",
    "            0.9993848046754845,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.9992881114614055,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.9990777743621273,\n",
    "            0.9993856864953414,\n",
    "            0.9995892380365579,\n",
    "            0.999591043860546,\n",
    "            0.9996930632289748,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.9996924969249692,\n",
    "    \"atom_wordlevel_50\": 0.9990821009688934,\n",
    "    \"smarts_wordlevel_106\": 0.9991833401388321,\n",
    "}\n",
    "novelty: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9467413441955194,\n",
    "    \"char_bpe_44\": 0.9446877864928186,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.9407544075440755,\n",
    "            0.9358567774936062,\n",
    "            0.9337225978283139,\n",
    "            0.9397454321494559,\n",
    "            0.9347959811359442,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.9302397049784881,\n",
    "            0.9310202820961598,\n",
    "            0.9360500923834941,\n",
    "            0.9343118136097711,\n",
    "            0.9348517492561814,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.9361421319796954,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.9376407945934876,\n",
    "            0.933620159803319,\n",
    "            0.9340850845720143,\n",
    "            0.9366283839212469,\n",
    "            0.9352621319380322,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.9384286586606961,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.936,\n",
    "            0.941399446777994,\n",
    "            0.939284980480789,\n",
    "            0.9391428863659609,\n",
    "            0.9359328625524511,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.9366348815749,\n",
    "    \"atom_wordlevel_50\": 0.9432421396488363,\n",
    "    \"smarts_wordlevel_106\": 0.9406416019615856,\n",
    "}\n",
    "fcd: Metric = {\n",
    "    \"char_wordlevel_38\": 0.22573631123455584,\n",
    "    \"char_bpe_44\": 0.23582004914318588,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.22660485100020367,\n",
    "            0.2283699198492286,\n",
    "            0.21916330554410024,\n",
    "            0.21487186290418947,\n",
    "            0.22354561820675656,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.22267958128769294,\n",
    "            0.2250078470330834,\n",
    "            0.21600954762025992,\n",
    "            0.21789636102157317,\n",
    "            0.21858163757028137,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.24258628303761043,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.21138082989205031,\n",
    "            0.22424112983600253,\n",
    "            0.22342369151375863,\n",
    "            0.21396691344180852,\n",
    "            0.22290331624026294,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.22958462926293066,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.22215763836744884,\n",
    "            0.21625658903371914,\n",
    "            0.21653028109176375,\n",
    "            0.22646688628489642,\n",
    "            0.23060648678460893,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.2323379188153183,\n",
    "    \"atom_wordlevel_50\": 0.23930838874149174,\n",
    "    \"smarts_wordlevel_106\": 0.24132050338971567,\n",
    "}\n",
    "\n",
    "fcd_guacamol = get_fcd_guacamol(fcd)\n",
    "\n",
    "guacamol_tokenizers = build_tokenizer_results(\n",
    "    GUACAMOL_TOKENIZERS,\n",
    "    validity=validity,\n",
    "    uniqueness=uniqueness,\n",
    "    novelty=novelty,\n",
    "    fcd=fcd,\n",
    "    fcd_guacamol=fcd_guacamol,\n",
    ")\n",
    "\n",
    "print_basic_stats(guacamol_tokenizers, sort_by=\"fcd\", reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.683389967Z",
     "start_time": "2023-11-24T11:24:00.595950929Z"
    }
   },
   "id": "9fe48796ddba5c55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizer Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9336aac4f2a5687"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Tokenizer Results - Which is the best tokenizer? ***\n",
      "\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric FCD with other tokenizers (in descending order)\n",
      "Alternative: LESS\n",
      "Best tokenizer: char_wordpiece_176 with metric mean 0.219 and std.dev. 0.006\n",
      "\n",
      "char_wordpiece_88    with metric single value 0.243 \n",
      "t-statistic: -8.677, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "smarts_wordlevel_106 with metric single value 0.241 \n",
      "t-statistic: -8.208, p-value:  0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_50    with metric single value 0.239 \n",
      "t-statistic: -7.462, p-value:  0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_44          with metric single value 0.236 \n",
      "t-statistic: -6.168, p-value:  0.002, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_176     with metric single value 0.232 \n",
      "t-statistic: -4.877, p-value:  0.004, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_44      with metric single value 0.230 \n",
      "t-statistic: -3.856, p-value:  0.009, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordlevel_38    with metric single value 0.226 \n",
      "t-statistic: -2.430, p-value:  0.036, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_88          with metric mean 0.223 and std.dev. 0.006\n",
      "t-statistic: -0.910, p-value:  0.195, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88      with metric mean 0.222 and std.dev. 0.006\n",
      "t-statistic: -0.829, p-value:  0.216, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_176         with metric mean 0.220 and std.dev. 0.004\n",
      "t-statistic: -0.269, p-value:  0.397, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordpiece_176   with metric mean 0.219 and std.dev. 0.006\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*** Tokenizer Results - Which is the best tokenizer? ***\\n\")\n",
    "criterion = \"fcd\"\n",
    "reverse = True\n",
    "alternative = \"less\"\n",
    "best_tokenizer, _ = get_best_tokenizer(\n",
    "    guacamol_tokenizers, sort_by=criterion, reverse=reverse\n",
    ")\n",
    "\n",
    "print_comparison(\n",
    "    best_tokenizer,\n",
    "    guacamol_tokenizers,\n",
    "    criterion=criterion,\n",
    "    alternative=alternative,\n",
    "    reverse=reverse,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.705866118Z",
     "start_time": "2023-11-24T11:24:00.596170090Z"
    }
   },
   "id": "34b7753e00e18512"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison with GuacaMol (and MolGPT)\n",
    "\n",
    "Regarding the FCD, we select char_wordpiece_176 to be the \"best\" tokenizer (with other tokenizers for which we can not reject the null hypothesis). We now compare it with the Guacamol metrics."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a7476e18f099fa4"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tokenizer char_wordpiece_176 has a mean of 0.219 and a std.dev. of 0.006\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "\n",
      "Paper GUACAMOL, metric VALIDITY, alternative: GREATER\n",
      "Compare our mean 0.976 and std.dev. 0.001 with single value of 0.959 in GuacaMol paper\n",
      "t-statistic: 36.835, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper GUACAMOL, metric UNIQUENESS, alternative: LESS\n",
      "Compare our mean 0.999 and std.dev. 0.000 with single value of 1.000 in GuacaMol paper\n",
      "t-statistic: -4.438, p-value: 0.006, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper GUACAMOL, metric NOVELTY, alternative: LESS\n",
      "Compare our mean 0.935 and std.dev. 0.002 with single value of 0.994 in GuacaMol paper\n",
      "t-statistic: -77.437, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper GUACAMOL, metric FCD, alternative: LESS\n",
      "Compare our mean 0.219 and std.dev. 0.006 with single value of 0.455 in GuacaMol paper\n",
      "t-statistic: -87.468, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper MOLGPT, metric VALIDITY, alternative: LESS\n",
      "Compare our mean 0.976 and std.dev. 0.001 with single value of 0.981 in MolGPT paper\n",
      "t-statistic: -9.899, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper MOLGPT, metric UNIQUENESS, alternative: GREATER\n",
      "Compare our mean 0.999 and std.dev. 0.000 with single value of 0.998 in MolGPT paper\n",
      "t-statistic: 7.947, p-value: 0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper MOLGPT, metric NOVELTY, alternative: LESS\n",
      "Compare our mean 0.935 and std.dev. 0.002 with single value of 1.000 in MolGPT paper\n",
      "t-statistic: -85.372, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Paper MOLGPT, metric FCD, alternative: LESS\n",
      "Compare our mean 0.219 and std.dev. 0.006 with single value of 0.488 in MolGPT paper\n",
      "t-statistic: -99.691, p-value: 0.000, Reject H0 (p<0.05): True\n"
     ]
    }
   ],
   "source": [
    "best_tokenizer, value = get_best_tokenizer(\n",
    "    guacamol_tokenizers, sort_by=\"fcd\", reverse=True\n",
    ")\n",
    "assert (\n",
    "    best_tokenizer == \"char_wordpiece_176\"\n",
    "), \"The best tokenizer has changed, check code/numbers!\"\n",
    "\n",
    "# This value is taken from the GuacaMol / MolGPT paper\n",
    "# Calculated as -5 log FCD\n",
    "\n",
    "guacamol_metrics: dict[str, float] = {\n",
    "    \"validity\": 0.959,\n",
    "    \"uniqueness\": 1.0,\n",
    "    \"novelty\": 0.994,\n",
    "    \"fcd\": 0.45509699193584,\n",
    "}\n",
    "molgpt_metrics: dict[str, float] = {\n",
    "    \"validity\": 0.981,\n",
    "    \"uniqueness\": 0.998,\n",
    "    \"novelty\": 1.0,\n",
    "    \"fcd\": 0.488064144335,\n",
    "}\n",
    "papers: dict[str, dict[str, float]] = {\n",
    "    \"GuacaMol\": guacamol_metrics,\n",
    "    \"MolGPT\": molgpt_metrics,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Selected tokenizer {best_tokenizer} has a mean of {value.mean:.3f} and a std.dev. of {value.std:.3f}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n*** Perform statistical tests ***\")\n",
    "\n",
    "for paper, compare_to in papers.items():\n",
    "    for metric_name, paper_value in compare_to.items():\n",
    "        # alternative = \"less\" if metric_name == \"fcd\" else \"greater\"\n",
    "        our_value = guacamol_tokenizers[best_tokenizer][metric_name]\n",
    "        our_mean = get_mean_value(our_value)\n",
    "        our_std = get_std_value(our_value)\n",
    "        alternative = \"less\" if our_mean < paper_value else \"greater\"\n",
    "        print(\n",
    "            f\"\\nPaper {paper.upper()}, metric {metric_name.upper()}, alternative: {alternative.upper()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Compare our mean {our_mean:.3f} and std.dev. {our_std:.3f} with single value of {paper_value:.3f} in {paper} paper\"\n",
    "        )\n",
    "        t_stat, p_val = t_test(our_value, paper_value, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:.3f}, \"\n",
    "            f\"p-value: {p_val:.3f}, \"\n",
    "            f\"Reject H0 (p<0.05): {p_val < ALPHA}\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:24:00.706173857Z",
     "start_time": "2023-11-24T11:24:00.639948082Z"
    }
   },
   "id": "2e97a46124536579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison with fine-tuned and hidden dim 768"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae85eeb6579ae74"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tokenizer char_wordpiece_176 has a mean of 0.219 and a std.dev. of 0.006\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "\n",
      "Experiment FINETUNED, metric VALIDITY, alternative: GREATER\n",
      "Compare our mean 0.976 and std.dev. 0.001 with experiment finetuned mean of 0.930 and std.dev. 0.003\n",
      "Warning: standard deviations differ by more than a factor of 2\n",
      "Therefore, we do a Welch's t-test instead of a Student's t-test!\n",
      "t-statistic: 35.746, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Experiment FINETUNED, metric UNIQUENESS, alternative: GREATER\n",
      "Compare our mean 0.999 and std.dev. 0.000 with experiment finetuned mean of 0.999 and std.dev. 0.000\n",
      "Warning: standard deviations differ by more than a factor of 2\n",
      "Therefore, we do a Welch's t-test instead of a Student's t-test!\n",
      "t-statistic: 2.818, p-value: 0.018, Reject H0 (p<0.05): True\n",
      "\n",
      "Experiment FINETUNED, metric NOVELTY, alternative: GREATER\n",
      "Compare our mean 0.935 and std.dev. 0.002 with experiment finetuned mean of 0.795 and std.dev. 0.008\n",
      "Warning: standard deviations differ by more than a factor of 2\n",
      "Therefore, we do a Welch's t-test instead of a Student's t-test!\n",
      "t-statistic: 38.879, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "Experiment FINETUNED, metric FCD, alternative: GREATER\n",
      "Compare our mean 0.219 and std.dev. 0.006 with experiment finetuned mean of 0.204 and std.dev. 0.005\n",
      "t-statistic: 4.155, p-value: 0.002, Reject H0 (p<0.05): True\n"
     ]
    }
   ],
   "source": [
    "best_tokenizer, value = get_best_tokenizer(\n",
    "    guacamol_tokenizers, sort_by=\"fcd\", reverse=True\n",
    ")\n",
    "assert (\n",
    "    best_tokenizer == \"char_wordpiece_176\"\n",
    "), \"The best tokenizer has changed, check code/numbers!\"\n",
    "\n",
    "finetuned_metrics: dict[str, Sample] = {\n",
    "    \"validity\": Sample(\n",
    "        [\n",
    "            0.9335,\n",
    "            0.926,\n",
    "            0.9296,\n",
    "            0.9308,\n",
    "            0.9293,\n",
    "        ]\n",
    "    ),\n",
    "    \"uniqueness\": Sample(\n",
    "        [\n",
    "            0.998821638993037,\n",
    "            0.9989200863930886,\n",
    "            0.9986015490533563,\n",
    "            0.9987107864202837,\n",
    "            0.9989239212310341,\n",
    "        ]\n",
    "    ),\n",
    "    \"novelty\": Sample(\n",
    "        [\n",
    "            0.7996567996567997,\n",
    "            0.804,\n",
    "            0.7913390067865992,\n",
    "            0.7833476764199656,\n",
    "            0.79446299687601,\n",
    "        ]\n",
    "    ),\n",
    "    \"fcd\": Sample(\n",
    "        [\n",
    "            0.20958906625868678,\n",
    "            0.21030092318062543,\n",
    "            0.20051053030638855,\n",
    "            0.19879689072276108,\n",
    "            0.20170148558332812,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "hidden_768_metrics: dict[str, Sample] = {\n",
    "    \"validity\": Sample(\n",
    "        [\n",
    "            0.965,\n",
    "            0.9713,\n",
    "            0.9564,\n",
    "            0.955,\n",
    "            0.9559,\n",
    "        ]\n",
    "    ),\n",
    "    \"uniqueness\": Sample(\n",
    "        [\n",
    "            0.9980310880829015,\n",
    "            0.9988674971687429,\n",
    "            0.9984316185696361,\n",
    "            0.9987434554973822,\n",
    "            0.9981169578407784,\n",
    "        ]\n",
    "    ),\n",
    "    \"novelty\": Sample(\n",
    "        [\n",
    "            0.7591112034056692,\n",
    "            0.7210884353741497,\n",
    "            0.7333752225363912,\n",
    "            0.7544558607674565,\n",
    "            0.7316843098207735,\n",
    "        ]\n",
    "    ),\n",
    "    \"fcd\": Sample(\n",
    "        [\n",
    "            0.2304540042858605,\n",
    "            0.22300828949749985,\n",
    "            0.2475916520934618,\n",
    "            0.2608303999207777,\n",
    "            0.2303283289356557,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "experiments: dict[str, dict[str, Sample]] = {\n",
    "    \"finetuned\": finetuned_metrics,\n",
    "    # \"hidden_768\": hidden_768_metrics,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Selected tokenizer {best_tokenizer} has a mean of {value.mean:.3f} and a std.dev. of {value.std:.3f}\"\n",
    ")\n",
    "\n",
    "print(f\"\\n*** Perform statistical tests ***\")\n",
    "\n",
    "for experiment, compare_to in experiments.items():\n",
    "    for metric_name, experiment_value in compare_to.items():\n",
    "        # alternative = \"less\" if metric_name == \"fcd\" else \"greater\"\n",
    "        our_value = guacamol_tokenizers[best_tokenizer][metric_name]\n",
    "        our_mean = get_mean_value(our_value)\n",
    "        our_std = get_std_value(our_value)\n",
    "        experiment_mean = get_mean_value(experiment_value)\n",
    "        experiment_std = get_std_value(experiment_value)\n",
    "        alternative = \"less\" if our_mean < experiment_mean else \"greater\"\n",
    "        print(\n",
    "            f\"\\nExperiment {experiment.upper()}, metric {metric_name.upper()}, alternative: {alternative.upper()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Compare our mean {our_mean:.3f} and std.dev. {our_std:.3f} with experiment {experiment} mean of {experiment_mean:.3f} and std.dev. {experiment_std:.3f}\"\n",
    "        )\n",
    "        t_stat, p_val = t_test(our_value, experiment_value, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:.3f}, \"\n",
    "            f\"p-value: {p_val:.3f}, \"\n",
    "            f\"Reject H0 (p<0.05): {p_val < ALPHA}\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:38:43.581321970Z",
     "start_time": "2023-11-24T11:38:43.498287946Z"
    }
   },
   "id": "1592278bb67e923b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criteria = [\"validity\", \"uniqueness\", \"novelty\", \"fcd\"]\n",
    "for criterion in criteria:\n",
    "    reverse = True if criterion == \"fcd\" else False\n",
    "    alternative = \"less\" if criterion == \"fcd\" else \"greater\"\n",
    "    best_tokenizer, _ = get_best_tokenizer(\n",
    "        guacamol_tokenizers, sort_by=criterion, reverse=reverse\n",
    "    )\n",
    "    print_comparison(\n",
    "        best_tokenizer,\n",
    "        guacamol_tokenizers,\n",
    "        criterion=criterion,\n",
    "        alternative=alternative,\n",
    "        reverse=reverse,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T11:24:00.656260416Z"
    }
   },
   "id": "be83e163ef5e532b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## USPTO50K/SMARTS\n",
    "\n",
    "We follow the same procedure as outlined in the GuacaMol section.\n",
    "- The initial \"best\" tokenizer: char_wordlevel_47\n",
    "- The \"candidate\" tokenizers are: char_bpe_47, char_bpe_88, atom_wordlevel_86, smarts_wordlevel_947\n",
    "- The final \"best\" tokenizer is: smarts_wordlevel_947"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bf7d52f64c2aa0d"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing basic stats for 10 tokenizers sorted by KNOWN_EITHER in ASCENDING order...\n",
      "\n",
      "*** CHAR_WORDPIECE_176 ***\n",
      "Metric: validity        Single value: 0.687\n",
      "Metric: uniqueness      Single value: 0.863\n",
      "Metric: feasibility     Single value: 0.088\n",
      "Metric: known_either    Single value: 557.000\n",
      "Metric: known_val       Single value: 385.000\n",
      "Metric: known_test      Single value: 396.000\n",
      "\n",
      "*** CHAR_WORDPIECE_94 ***\n",
      "Metric: validity        Single value: 0.731\n",
      "Metric: uniqueness      Single value: 0.880\n",
      "Metric: feasibility     Single value: 0.088\n",
      "Metric: known_either    Single value: 587.000\n",
      "Metric: known_val       Single value: 406.000\n",
      "Metric: known_test      Single value: 409.000\n",
      "\n",
      "*** CHAR_UNIGRAM_176 ***\n",
      "Metric: validity        Single value: 0.712\n",
      "Metric: uniqueness      Single value: 0.811\n",
      "Metric: feasibility     Single value: 0.100\n",
      "Metric: known_either    Single value: 616.000\n",
      "Metric: known_val       Single value: 416.000\n",
      "Metric: known_test      Single value: 395.000\n",
      "\n",
      "*** CHAR_BPE_176 ***\n",
      "Metric: validity        Single value: 0.688\n",
      "Metric: uniqueness      Single value: 0.845\n",
      "Metric: feasibility     Single value: 0.098\n",
      "Metric: known_either    Single value: 618.000\n",
      "Metric: known_val       Single value: 410.000\n",
      "Metric: known_test      Single value: 431.000\n",
      "\n",
      "*** CHAR_UNIGRAM_88 ***\n",
      "Metric: validity        Single value: 0.720\n",
      "Metric: uniqueness      Single value: 0.852\n",
      "Metric: feasibility     Single value: 0.096\n",
      "Metric: known_either    Single value: 621.000\n",
      "Metric: known_val       Single value: 424.000\n",
      "Metric: known_test      Single value: 433.000\n",
      "\n",
      "*** CHAR_BPE_88 ***\n",
      "Metric: validity        Mean:         0.735   Std.dev. 0.024\n",
      "Metric: uniqueness      Mean:         0.835   Std.dev. 0.006\n",
      "Metric: feasibility     Mean:         0.104   Std.dev. 0.004\n",
      "Metric: known_either    Mean:         688.000   Std.dev. 35.686\n",
      "Metric: known_val       Mean:         464.200   Std.dev. 32.275\n",
      "Metric: known_test      Mean:         476.600   Std.dev. 22.733\n",
      "\n",
      "*** CHAR_BPE_47 ***\n",
      "Metric: validity        Mean:         0.762   Std.dev. 0.010\n",
      "Metric: uniqueness      Mean:         0.850   Std.dev. 0.018\n",
      "Metric: feasibility     Mean:         0.102   Std.dev. 0.005\n",
      "Metric: known_either    Mean:         698.400   Std.dev. 38.384\n",
      "Metric: known_val       Mean:         477.200   Std.dev. 27.225\n",
      "Metric: known_test      Mean:         477.600   Std.dev. 27.245\n",
      "\n",
      "*** CHAR_WORDLEVEL_47 ***\n",
      "Metric: validity        Mean:         0.764   Std.dev. 0.010\n",
      "Metric: uniqueness      Mean:         0.849   Std.dev. 0.008\n",
      "Metric: feasibility     Mean:         0.104   Std.dev. 0.003\n",
      "Metric: known_either    Mean:         705.200   Std.dev. 21.230\n",
      "Metric: known_val       Mean:         489.200   Std.dev. 14.772\n",
      "Metric: known_test      Mean:         491.400   Std.dev. 9.711\n",
      "\n",
      "*** ATOM_WORDLEVEL_86 ***\n",
      "Metric: validity        Mean:         0.755   Std.dev. 0.024\n",
      "Metric: uniqueness      Mean:         0.846   Std.dev. 0.017\n",
      "Metric: feasibility     Mean:         0.108   Std.dev. 0.006\n",
      "Metric: known_either    Mean:         730.800   Std.dev. 52.713\n",
      "Metric: known_val       Mean:         496.000   Std.dev. 38.295\n",
      "Metric: known_test      Mean:         503.800   Std.dev. 35.031\n",
      "\n",
      "*** SMARTS_WORDLEVEL_947 ***\n",
      "Metric: validity        Mean:         0.804   Std.dev. 0.022\n",
      "Metric: uniqueness      Mean:         0.795   Std.dev. 0.008\n",
      "Metric: feasibility     Mean:         0.110   Std.dev. 0.004\n",
      "Metric: known_either    Mean:         735.200   Std.dev. 27.041\n",
      "Metric: known_val       Mean:         489.600   Std.dev. 13.576\n",
      "Metric: known_test      Mean:         511.000   Std.dev. 22.572\n"
     ]
    }
   ],
   "source": [
    "USPTO50K_TOKENIZERS: Final[set[str]] = {\n",
    "    \"char_wordlevel_47\",\n",
    "    \"char_bpe_47\",\n",
    "    \"char_bpe_88\",\n",
    "    \"char_bpe_176\",\n",
    "    \"char_wordpiece_94\",\n",
    "    \"char_wordpiece_176\",\n",
    "    \"char_unigram_88\",\n",
    "    \"char_unigram_176\",\n",
    "    \"atom_wordlevel_86\",\n",
    "    \"smarts_wordlevel_947\",\n",
    "}\n",
    "validity: Metric = {\n",
    "    \"char_wordlevel_47\": Sample(\n",
    "        [\n",
    "            0.7564622402432843,\n",
    "            0.7521546261089987,\n",
    "            0.7727957059632127,\n",
    "            0.7647479674796748,\n",
    "            0.773511075739488,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_47\": Sample(\n",
    "        [\n",
    "            0.7526333006215243,\n",
    "            0.7697592253336822,\n",
    "            0.7536166365280289,\n",
    "            0.7747238633444644,\n",
    "            0.7591536804763714,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.734920634920635,\n",
    "            0.7273118279569892,\n",
    "            0.7357945819767799,\n",
    "            0.7056446890736621,\n",
    "            0.7728643861112894,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": 0.6884913743197869,\n",
    "    \"char_wordpiece_94\": 0.731087584215592,\n",
    "    \"char_wordpiece_176\": 0.6867148613415501,\n",
    "    \"char_unigram_88\": 0.7195553926957371,\n",
    "    \"char_unigram_176\": 0.7116800920598388,\n",
    "    \"atom_wordlevel_86\": Sample(\n",
    "        [\n",
    "            0.7702271567126084,\n",
    "            0.7819990954319312,\n",
    "            0.7195848430878831,\n",
    "            0.7434729617037654,\n",
    "            0.7591343992808065,\n",
    "        ]\n",
    "    ),\n",
    "    \"smarts_wordlevel_947\": Sample(\n",
    "        [\n",
    "            0.8099435601496607,\n",
    "            0.7849000186880957,\n",
    "            0.8018752020691885,\n",
    "            0.7866085331672377,\n",
    "            0.838874510584644,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "uniqueness: Metric = {\n",
    "    \"char_wordlevel_47\": Sample(\n",
    "        [\n",
    "            0.8380234505862647,\n",
    "            0.8474176425983655,\n",
    "            0.8493139081822801,\n",
    "            0.8511651641435618,\n",
    "            0.8606704964417389,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_47\": Sample(\n",
    "        [\n",
    "            0.8739568845618915,\n",
    "            0.8506587335316617,\n",
    "            0.8609135315793984,\n",
    "            0.8315649867374005,\n",
    "            0.8349465954606141,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.8315334773218143,\n",
    "            0.8449776125707528,\n",
    "            0.8374519953247621,\n",
    "            0.8317152103559871,\n",
    "            0.8309250954990866,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": 0.8446985621794333,\n",
    "    \"char_wordpiece_94\": 0.8800245743373706,\n",
    "    \"char_wordpiece_176\": 0.8634912416947105,\n",
    "    \"char_unigram_88\": 0.8515532167713461,\n",
    "    \"char_unigram_176\": 0.8112216023930795,\n",
    "    \"atom_wordlevel_86\": Sample(\n",
    "        [\n",
    "            0.830668544445365,\n",
    "            0.8267371726018342,\n",
    "            0.855509089357344,\n",
    "            0.8660051768766178,\n",
    "            0.8486719675181864,\n",
    "        ]\n",
    "    ),\n",
    "    \"smarts_wordlevel_947\": Sample(\n",
    "        [\n",
    "            0.7864860632633887,\n",
    "            0.7954761904761904,\n",
    "            0.8079993548907346,\n",
    "            0.7938870852799114,\n",
    "            0.7912348706589668,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "feasibility: Metric = {\n",
    "    \"char_wordlevel_47\": Sample(\n",
    "        [\n",
    "            0.10433739756146312,\n",
    "            0.10866971564923444,\n",
    "            0.09913234267477809,\n",
    "            0.10281774580335731,\n",
    "            0.1026100816895796,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_47\": Sample(\n",
    "        [\n",
    "            0.09757310523174856,\n",
    "            0.10331734612310152,\n",
    "            0.0970535536531953,\n",
    "            0.10287081339712918,\n",
    "            0.11023385968418949,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.1047952047952048,\n",
    "            0.09878024395120975,\n",
    "            0.10577210646994317,\n",
    "            0.10116731517509728,\n",
    "            0.10893463921647012,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": 0.09755126418475014,\n",
    "    \"char_wordpiece_94\": 0.08766330906552308,\n",
    "    \"char_wordpiece_176\": 0.08843809333466573,\n",
    "    \"char_unigram_88\": 0.09608292634306788,\n",
    "    \"char_unigram_176\": 0.10025911899541559,\n",
    "    \"atom_wordlevel_86\": Sample(\n",
    "        [\n",
    "            0.11229679864366211,\n",
    "            0.11563062162702378,\n",
    "            0.10085794094173982,\n",
    "            0.1026203048719737,\n",
    "            0.10814312767866043,\n",
    "        ]\n",
    "    ),\n",
    "    \"smarts_wordlevel_947\": Sample(\n",
    "        [\n",
    "            0.11428571428571428,\n",
    "            0.10426020153646613,\n",
    "            0.10968063872255489,\n",
    "            0.10941551964891283,\n",
    "            0.11127774445110977,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "known_either: Metric = {\n",
    "    \"char_wordlevel_47\": Sample([728.0, 726.0, 679.0, 699.0, 694.0]),\n",
    "    \"char_bpe_47\": Sample([663.0, 686.0, 686.0, 693.0, 764.0]),\n",
    "    \"char_bpe_88\": Sample([691.0, 631.0, 694.0, 694.0, 730.0]),\n",
    "    \"char_bpe_176\": 618.0,\n",
    "    \"char_wordpiece_94\": 587.0,\n",
    "    \"char_wordpiece_176\": 557.0,\n",
    "    \"char_unigram_88\": 621.0,\n",
    "    \"char_unigram_176\": 616.0,\n",
    "    \"atom_wordlevel_86\": Sample([775.0, 782.0, 670.0, 680.0, 747.0]),\n",
    "    \"smarts_wordlevel_947\": Sample([746.0, 699.0, 771.0, 721.0, 739.0]),\n",
    "}\n",
    "known_val: Metric = {\n",
    "    \"char_wordlevel_47\": Sample([498.0, 499.0, 467.0, 481.0, 501.0]),\n",
    "    \"char_bpe_47\": Sample([453.0, 467.0, 473.0, 469.0, 524.0]),\n",
    "    \"char_bpe_88\": Sample([456.0, 413.0, 475.0, 479.0, 498.0]),\n",
    "    \"char_bpe_176\": 410.0,\n",
    "    \"char_wordpiece_94\": 406.0,\n",
    "    \"char_wordpiece_176\": 385.0,\n",
    "    \"char_unigram_88\": 424.0,\n",
    "    \"char_unigram_176\": 416.0,\n",
    "    \"atom_wordlevel_86\": Sample([536.0, 525.0, 455.0, 456.0, 508.0]),\n",
    "    \"smarts_wordlevel_947\": Sample([500.0, 479.0, 508.0, 478.0, 483.0]),\n",
    "}\n",
    "known_test: Metric = {\n",
    "    \"char_wordlevel_47\": Sample([503.0, 493.0, 478.0, 497.0, 486.0]),\n",
    "    \"char_bpe_47\": Sample([465.0, 466.0, 461.0, 470.0, 526.0]),\n",
    "    \"char_bpe_88\": Sample([490.0, 438.0, 478.0, 481.0, 496.0]),\n",
    "    \"char_bpe_176\": 431.0,\n",
    "    \"char_wordpiece_94\": 409.0,\n",
    "    \"char_wordpiece_176\": 396.0,\n",
    "    \"char_unigram_88\": 433.0,\n",
    "    \"char_unigram_176\": 395.0,\n",
    "    \"atom_wordlevel_86\": Sample([543.0, 527.0, 467.0, 467.0, 515.0]),\n",
    "    \"smarts_wordlevel_947\": Sample([519.0, 477.0, 532.0, 500.0, 527.0]),\n",
    "}\n",
    "\n",
    "uspto50k_tokenizers = build_tokenizer_results(\n",
    "    USPTO50K_TOKENIZERS,\n",
    "    validity=validity,\n",
    "    uniqueness=uniqueness,\n",
    "    feasibility=feasibility,\n",
    "    known_either=known_either,\n",
    "    known_val=known_val,\n",
    "    known_test=known_test,\n",
    ")\n",
    "\n",
    "print_basic_stats(uspto50k_tokenizers, sort_by=\"known_either\", reverse=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:42:05.305576871Z",
     "start_time": "2023-11-24T11:42:05.271560774Z"
    }
   },
   "id": "ac316a26b4c0fd6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenizer Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebaa0453388f0e34"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Tokenizer Results - Which is the best tokenizer? ***\n",
      "\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric KNOWN_EITHER with other tokenizers (in ascending order)\n",
      "Alternative: GREATER\n",
      "Best tokenizer: smarts_wordlevel_947 with metric mean 735.200 and std.dev. 27.041\n",
      "\n",
      "char_wordpiece_176   with metric single value 557.000 \n",
      "t-statistic: 14.736, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordpiece_94    with metric single value 587.000 \n",
      "t-statistic: 12.255, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_176     with metric single value 616.000 \n",
      "t-statistic:  9.857, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_176         with metric single value 618.000 \n",
      "t-statistic:  9.692, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_88      with metric single value 621.000 \n",
      "t-statistic:  9.444, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_88          with metric mean 688.000 and std.dev. 35.686\n",
      "t-statistic:  2.357, p-value:  0.023, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_47          with metric mean 698.400 and std.dev. 38.384\n",
      "t-statistic:  1.753, p-value:  0.059, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordlevel_47    with metric mean 705.200 and std.dev. 21.230\n",
      "t-statistic:  1.951, p-value:  0.043, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_86    with metric mean 730.800 and std.dev. 52.713\n",
      "t-statistic:  0.166, p-value:  0.436, Reject H0 (p<0.05): False\n",
      "\n",
      "smarts_wordlevel_947 with metric mean 735.200 and std.dev. 27.041\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n*** Tokenizer Results - Which is the best tokenizer? ***\\n\")\n",
    "criterion = \"known_either\"\n",
    "reverse = False\n",
    "alternative = \"greater\"\n",
    "best_tokenizer, _ = get_best_tokenizer(\n",
    "    uspto50k_tokenizers, sort_by=criterion, reverse=reverse\n",
    ")\n",
    "\n",
    "print_comparison(\n",
    "    best_tokenizer,\n",
    "    uspto50k_tokenizers,\n",
    "    criterion=criterion,\n",
    "    alternative=alternative,\n",
    "    reverse=reverse,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T11:45:14.969534166Z",
     "start_time": "2023-11-24T11:45:14.908222006Z"
    }
   },
   "id": "2e70f8de5be8b1b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Archive\n",
    "\n",
    "The following cells have been used to check my understanding of the statistical tests and the scipy package."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbab5696061a5be1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_two_samples(s1: Sample, s2: Sample) -> tuple[float, float]:\n",
    "    t_stat, p_value = ttest_ind(s1.array, s2.array)\n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "# Same calculation for both one and two sample t-tests\n",
    "def _calculate_p_value(t_statistic: float, df: int, alternative: str) -> float:\n",
    "    alternative = alternative.strip().lower()\n",
    "    if alternative == \"two_sided\":\n",
    "        p_value = (1 - t_statistic.cdf(abs(t_statistic), df)) * 2.0\n",
    "    elif alternative == \"less\":\n",
    "        p_value = t_statistic.cdf(t_statistic, df)\n",
    "    elif alternative == \"greater\":\n",
    "        p_value = 1 - t_statistic.cdf(t_statistic, df)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two_sided', 'greater' or 'less'\")\n",
    "\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T11:24:00.704169900Z"
    }
   },
   "id": "27019476b7749854"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
