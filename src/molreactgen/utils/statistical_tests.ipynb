{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a6405014065f5e"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Iterable, Optional, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, ttest_1samp, ttest_ind  # t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:57:20.574282235Z",
     "start_time": "2023-11-13T11:57:20.543869084Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ALTERNATIVES = (\"two-sided\", \"less\", \"greater\")\n",
    "ALPHA = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.273139763Z",
     "start_time": "2023-11-13T11:26:21.272720910Z"
    }
   },
   "id": "444ab0135c733968"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    values: Sequence[float]\n",
    "    array: np.ndarray = None\n",
    "    ddof: int = 1\n",
    "    mean: float = None\n",
    "    std: float = None\n",
    "    variance: float = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.array = np.array(self.values)\n",
    "        self.mean, self.std, self.var = self.get_basic_stats()\n",
    "\n",
    "    def get_basic_stats(self) -> tuple[float, float, float]:\n",
    "        mean = np.mean(self.array)\n",
    "        std = np.std(self.array, ddof=self.ddof)\n",
    "        var = np.var(self.array, ddof=self.ddof)\n",
    "        return mean, std, var\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Sample({self.values}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.array})\"\n",
    "\n",
    "\n",
    "MetricValue = Optional[Union[float, Sample]]\n",
    "Metric = dict[str, MetricValue]\n",
    "TokenizerResults = dict[str, Metric]\n",
    "\n",
    "# @dataclass\n",
    "# class GuacaMolTokenizerResult:\n",
    "#     # tokenizer: str\n",
    "#     validity: Metric = None\n",
    "#     uniqueness: Metric = None\n",
    "#     novelty: Metric = None\n",
    "#     fcd: Metric = None\n",
    "#     # fcd_guacamol: Metric = None\n",
    "#\n",
    "#     @property\n",
    "#     def fcd_guacamol(self) -> Metric:\n",
    "#         if isinstance(self.fcd, float):\n",
    "#             return -5. * np.log(self.fcd)\n",
    "#         elif isinstance(self.fcd, Sample):\n",
    "#             values = [-5. * np.log(fcd) for fcd in self.fcd.values]\n",
    "#             return Sample(values)\n",
    "#         else:\n",
    "#             raise TypeError(\"Wrong type for fcd\")\n",
    "\n",
    "\n",
    "def build_guacamol_tokenizer_results(\n",
    "    tokenizer_names: Iterable[str],\n",
    "    validity_results: Iterable[Metric],\n",
    "    uniqueness_results: Iterable[Metric],\n",
    "    novelty_results: Iterable[Metric],\n",
    "    fcd_results: Iterable[Metric],\n",
    ") -> TokenizerResults:\n",
    "    # results: dict[str, GuacaMolTokenizerResult] = {}\n",
    "    # for name, validity, uniqueness, novelty, fcd in zip(tokenizer_names, validity_results, uniqueness_results, novelty_results, fcd_results):\n",
    "    #     results[name] = GuacaMolTokenizerResult(validity, uniqueness, novelty, fcd)\n",
    "    results: TokenizerResults = {}\n",
    "    for tokenizer_name, validity, uniqueness, novelty, fcd in zip(\n",
    "        tokenizer_names,\n",
    "        validity_results,\n",
    "        uniqueness_results,\n",
    "        novelty_results,\n",
    "        fcd_results,\n",
    "    ):\n",
    "        results[tokenizer_name] = {\n",
    "            \"validity\": validity,\n",
    "            \"uniqueness\": uniqueness,\n",
    "            \"novelty\": novelty,\n",
    "            \"fcd\": fcd,\n",
    "        }\n",
    "    for tokenizer_name, fcd in zip(tokenizer_names, fcd_results):\n",
    "        if isinstance(fcd, float):\n",
    "            results[tokenizer_name][\"fcd_guacamol\"] = np.exp(-0.2 * fcd)\n",
    "        elif isinstance(fcd, Sample):\n",
    "            values = [np.exp(-0.2 * fcd) for fcd in fcd.values]\n",
    "            results[tokenizer_name][\"fcd_guacamol\"] = Sample(values)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_mean_value(result: MetricValue) -> float:\n",
    "    if isinstance(result, float):\n",
    "        return result\n",
    "    elif isinstance(result, Sample):\n",
    "        return result.mean\n",
    "    elif result is None:\n",
    "        return None\n",
    "    else:\n",
    "        raise ValueError(\"result must be a float or a Sample\")\n",
    "\n",
    "\n",
    "def print_basic_stats(\n",
    "    results: TokenizerResults, sort_by: str = \"fcd\", reverse: bool = False\n",
    "):\n",
    "    # for tokenizer_name, result in sorted(results.items(), key=lambda x: get_mean_value(x[1]), reverse=reverse):\n",
    "    reverse_string = \"descending\" if reverse else \"ascending\"\n",
    "    print(\n",
    "        f\"Printing basic stats for {len(results)} tokenizers sorted by {sort_by.upper()} in {reverse_string.upper()} order...\"\n",
    "    )\n",
    "    for tokenizer_name in sorted(\n",
    "        results, key=lambda x: get_mean_value(results[x][sort_by]), reverse=reverse\n",
    "    ):\n",
    "        print(f\"\\n*** {tokenizer_name.upper()} ***\")\n",
    "        for metric_name, value in results[tokenizer_name].items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"Metric: {metric_name:15s} Single value: {value:.3f}\")\n",
    "            elif isinstance(value, Sample):\n",
    "                print(\n",
    "                    f\"Metric: {metric_name:15s} Mean:         {value.mean:.3f}   Std.dev. {value.std:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                raise TypeError(\"results must be a dict[str, dict[float|Sample]]\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:01:24.328082934Z",
     "start_time": "2023-11-13T13:01:24.281605107Z"
    }
   },
   "id": "2e1f4a4d72352cdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The statistical tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be786b18c8ea5098"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def is_normally_distributed(sample: Sample, alpha: float = ALPHA):\n",
    "    _, p_value = shapiro(sample.array)\n",
    "    return p_value > alpha"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.274108265Z",
     "start_time": "2023-11-13T11:26:21.273252134Z"
    }
   },
   "id": "c0d4cdc77bbde306"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def one_sample_t_test(\n",
    "    sample: Sample, population_mean: float, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    # n = len(sample)\n",
    "    # df = n - 1\n",
    "    # t_statistic = (sample.mean - population_mean) / (sample.std / np.sqrt(n))\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    test_statistic, p_value = ttest_1samp(\n",
    "        sample.array, population_mean, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.275695985Z",
     "start_time": "2023-11-13T11:26:21.273494287Z"
    }
   },
   "id": "d2ce292f1d0d6014"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def two_sample_t_test(\n",
    "    sample: Sample, baseline: Sample, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    # n1 = len(sample)\n",
    "    # n2 = len(baseline)\n",
    "    if sample.std > 2.0 * baseline.std or baseline.std > 2.0 * sample.std:\n",
    "        print(\"Warning: standard deviations differ by more than a factor of 2\")\n",
    "        print(\"Therefore, we do a Welch's t-test instead of a Student's t-test!\")\n",
    "        equal_var = False\n",
    "    else:\n",
    "        equal_var = True\n",
    "\n",
    "    # pooled_std = np.sqrt(((n1 - 1) * sample.std ** 2 + (n2 - 1) * baseline.std ** 2) / (n1 + n2 - 2))\n",
    "    # t_statistic = (sample.mean - baseline.mean) / (pooled_std * np.sqrt(1 / n1 + 1 / n2))\n",
    "    # df = n1 + n2 - 2\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    if not is_normally_distributed(baseline):\n",
    "        print(\"Warning: baseline is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "\n",
    "    test_statistic, p_value = ttest_ind(\n",
    "        sample.array, baseline.array, equal_var=equal_var, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.291031180Z",
     "start_time": "2023-11-13T11:26:21.276772569Z"
    }
   },
   "id": "1eaaa267aedff3ba"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def interpret_p_value(\n",
    "    p_value: float,\n",
    "    # alternative: str,\n",
    "    alpha: float = ALPHA,\n",
    ") -> str:\n",
    "    if p_value < alpha:\n",
    "        return \"Reject H0\"\n",
    "    else:\n",
    "        return \"Cannot reject H0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.308756062Z",
     "start_time": "2023-11-13T11:26:21.291337158Z"
    }
   },
   "id": "46ab88ffb3d0f902"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def t_test(\n",
    "    sample: Sample, comparison: Union[float, Sample], alternative: str = \"two_sided\"\n",
    "):\n",
    "    if isinstance(comparison, float):\n",
    "        t_statistic, p_value = one_sample_t_test(sample, comparison, alternative)\n",
    "    elif isinstance(comparison, Sample):\n",
    "        t_statistic, p_value = two_sample_t_test(sample, comparison, alternative)\n",
    "    else:\n",
    "        raise ValueError(\"comparison must be a float or a Sample\")\n",
    "\n",
    "    return t_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.316961603Z",
     "start_time": "2023-11-13T11:26:21.307168718Z"
    }
   },
   "id": "91c8db71576e9a1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279ffc159089ec11"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test example\n",
      "Sample is normally distributed: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapiro-Wilk test example\")\n",
    "sample = Sample([2.5, 3.1, 2.8, 3.4, 2.9, 3.0, 3.3, 2.6, 3.2, 3.1])\n",
    "result = is_normally_distributed(sample)\n",
    "print(f\"Sample is normally distributed: {result}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.334476267Z",
     "start_time": "2023-11-13T11:26:21.317404087Z"
    }
   },
   "id": "fbf7881d94b693a0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample t-test example\n",
      "Sample: [10. 11. 12. 13. 14.])\n",
      "Population mean: 14.0\n",
      "Alternative: two-sided, t-statistic: -2.828, p-value: 0.047, Reject H0 (p<0.05): True\n",
      "Alternative: less     , t-statistic: -2.828, p-value: 0.024, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.828, p-value: 0.976, Reject H0 (p<0.05): False\n",
      "\n",
      "Two sample t-test example\n",
      "Sample: [10. 11. 12.])\n",
      "Baseline: [12. 13. 14.])\n",
      "Alternative: two-sided, t-statistic: -2.449, p-value: 0.070, Reject H0 (p<0.05): False\n",
      "Alternative: less     , t-statistic: -2.449, p-value: 0.035, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.449, p-value: 0.965, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"One sample t-test example\")\n",
    "sample = Sample([10.0, 11.0, 12.0, 13.0, 14.0])\n",
    "population_mean = 14.0\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Population mean: {population_mean}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = one_sample_t_test(sample, population_mean, alternative)\n",
    "    t_stat, p_val = t_test(sample, population_mean, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTwo sample t-test example\")\n",
    "sample = Sample(np.array([10.0, 11.0, 12.0]))\n",
    "baseline = Sample(np.array([12.0, 13.0, 14.0]))\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Baseline: {baseline}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = two_sample_t_test(sample, baseline, alternative)\n",
    "    t_stat, p_val = t_test(sample, baseline, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T11:26:21.333350732Z"
    }
   },
   "id": "84baad5a7392d8a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizer Experiments\n",
    "\n",
    "## Guacamol/SMILES\n",
    "\n",
    "### The initial \"best\" tokenizer\n",
    "\n",
    "This is based on a single run for each tokenizer.\n",
    "A single run consists of:\n",
    "\n",
    "- a training with fixed seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb8d2d2b2f7ebb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical tests for the \"best\" tokenizer\n",
    "\n",
    "This is based on 5 runs for the \"best\" tokenizer.\n",
    "The 5 runs are:\n",
    "\n",
    "- a training with random seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric\n",
    "\n",
    "At this point we have 5 FCD values for the \"best\" tokenizer.\n",
    "We then do a one-sample t-test and compare the sample with the \"next best\" tokenizer. If the current \"best\" tokenizer can be considered \"done\" and we declare the \"best\" tokenizer as the \"winner\". If not, we repeat the process with the \"next best\" tokenizer and perform a two-sample t-test. We repeat this process until we have a winner, or, we conculude that we have a couple of tokenizers which seem to perform equally well.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c7f589001ae7dc"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "tokenizers = [\n",
    "    \"char_wordlevel_38\",\n",
    "    \"char_bpe_44\",\n",
    "    \"char_bpe_88\",\n",
    "    \"char_bpe_176\",\n",
    "    \"char_wordpiece_88\",\n",
    "    \"char_wordpiece_176\",\n",
    "    \"char_unigram_44\",\n",
    "    \"char_unigram_88\",\n",
    "    \"char_unigram_176\",\n",
    "    \"atom_wordlevel_50\",\n",
    "    \"smarts_wordlevel_106\",\n",
    "]\n",
    "\n",
    "validity_results: list[Metric] = [0.0] * len(tokenizers)\n",
    "uniqueness_results: list[Metric] = [0.0] * len(tokenizers)\n",
    "novelty_results: list[Metric] = [0.0] * len(tokenizers)\n",
    "fcd_results: list[Metric] = [\n",
    "    0.22573631123455584,\n",
    "    0.23582004914318588,\n",
    "    0.22138152297816305,\n",
    "    0.22446600524058624,\n",
    "    0.24258628303761043,\n",
    "    Sample(\n",
    "        [\n",
    "            0.21138082989205031,\n",
    "            0.22424112983600253,\n",
    "            0.22342369151375863,\n",
    "            0.21396691344180852,\n",
    "            0.22290331624026294,\n",
    "        ]\n",
    "    ),\n",
    "    0.22958462926293066,\n",
    "    Sample(\n",
    "        [\n",
    "            0.22215763836744884,\n",
    "            0.21625658903371914,\n",
    "            0.21653028109176375,\n",
    "            0.22646688628489642,\n",
    "            0.23060648678460893,\n",
    "        ]\n",
    "    ),\n",
    "    0.2323379188153183,\n",
    "    0.23930838874149174,\n",
    "    0.24132050338971567,\n",
    "]\n",
    "\n",
    "assert (\n",
    "    len(tokenizers)\n",
    "    == len(validity_results)\n",
    "    == len(uniqueness_results)\n",
    "    == len(novelty_results)\n",
    "    == len(fcd_results)\n",
    ")\n",
    "\n",
    "guacamol_tokenizers = build_guacamol_tokenizer_results(\n",
    "    tokenizers, validity_results, uniqueness_results, novelty_results, fcd_results\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:16:12.882849271Z",
     "start_time": "2023-11-13T13:16:12.809900852Z"
    }
   },
   "id": "9fe48796ddba5c55"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing basic stats for 11 tokenizers sorted by FCD in DESCENDING order...\n",
      "\n",
      "*** CHAR_WORDPIECE_88 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.243\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** SMARTS_WORDLEVEL_106 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.241\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** ATOM_WORDLEVEL_50 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.239\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** CHAR_BPE_44 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.236\n",
      "Metric: fcd_guacamol    Single value: 0.954\n",
      "\n",
      "*** CHAR_UNIGRAM_176 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.232\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_UNIGRAM_44 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.230\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_WORDLEVEL_38 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.226\n",
      "Metric: fcd_guacamol    Single value: 0.956\n",
      "\n",
      "*** CHAR_BPE_176 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.224\n",
      "Metric: fcd_guacamol    Single value: 0.956\n",
      "\n",
      "*** CHAR_UNIGRAM_88 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Mean:         0.222   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.956   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_BPE_88 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Single value: 0.221\n",
      "Metric: fcd_guacamol    Single value: 0.957\n",
      "\n",
      "*** CHAR_WORDPIECE_176 ***\n",
      "Metric: validity        Single value: 0.000\n",
      "Metric: uniqueness      Single value: 0.000\n",
      "Metric: novelty         Single value: 0.000\n",
      "Metric: fcd             Mean:         0.219   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.957   Std.dev. 0.001\n"
     ]
    }
   ],
   "source": [
    "print_basic_stats(guacamol_tokenizers, reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:16:19.006249478Z",
     "start_time": "2023-11-13T13:16:19.000047061Z"
    }
   },
   "id": "f7a7b5e2286e9f8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bdec98cc662559a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial best tokenizer char_wordpiece_176 has a mean of 0.2192 and a std.dev. of 0.0060\n",
      "\n",
      "Compare with single FCD value of other tokenizers (descending by FCD)\n",
      "Perform statistical test... (alternative: less)\n",
      "\n",
      "char_wordpiece_88   : 0.2426\n",
      "t-statistic: -8.677, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "smarts_wordlevel_106: 0.2413\n",
      "t-statistic: -8.208, p-value: 0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_50   : 0.2393\n",
      "t-statistic: -7.462, p-value: 0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_44         : 0.2358\n",
      "t-statistic: -6.168, p-value: 0.002, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_44     : 0.2296\n",
      "t-statistic: -3.856, p-value: 0.009, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordlevel_38   : 0.2257\n",
      "t-statistic: -2.430, p-value: 0.036, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_176        : 0.2245\n",
      "t-statistic: -1.959, p-value: 0.061, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_88         : 0.2214\n",
      "t-statistic: -0.815, p-value: 0.230, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88     : 0.2189\n",
      "t-statistic: 0.088, p-value: 0.533, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "guacamol_char_wordpiece_176 = Sample(\n",
    "    [\n",
    "        0.21138082989205031,\n",
    "        0.22424112983600253,\n",
    "        0.22342369151375863,\n",
    "        0.21396691344180852,\n",
    "        0.22290331624026294,\n",
    "    ]\n",
    ")\n",
    "\n",
    "guacamol_others: dict[str, float] = {\n",
    "    \"char_wordlevel_38\": 0.22573631123455584,\n",
    "    \"char_bpe_44\": 0.23582004914318588,\n",
    "    \"char_bpe_88\": 0.22138152297816305,\n",
    "    \"char_bpe_176\": 0.22446600524058624,\n",
    "    \"char_wordpiece_88\": 0.24258628303761043,\n",
    "    \"char_unigram_44\": 0.22958462926293066,\n",
    "    \"char_unigram_88\": 0.2189468387861666,\n",
    "    \"atom_wordlevel_50\": 0.23930838874149174,\n",
    "    \"smarts_wordlevel_106\": 0.24132050338971567,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Initial best tokenizer char_wordpiece_176 has a mean of {guacamol_char_wordpiece_176.mean:.4f} and a std.dev. of {guacamol_char_wordpiece_176.std:.4f}\"\n",
    ")\n",
    "print(\"\\nCompare with single FCD value of other tokenizers (descending by FCD)\")\n",
    "alternative = \"less\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "\n",
    "for tokenizer, fcd in sorted(guacamol_others.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"\\n{tokenizer:20s}: {fcd:.4f}\")\n",
    "    t_stat, p_val = one_sample_t_test(guacamol_char_wordpiece_176, fcd, alternative)\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T11:26:21.348388351Z"
    }
   },
   "id": "3c9ca98f81096f3f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results for the initial \"best\" tokenizer\n",
    "\n",
    "The initial \"best\" tokenizer is char_wordpiece_176. After 5 runs and 5 FCD values we see, that the mean of those 5 values is worse than the single char_unigram_88 value. Therefore, we can expect that the char_wordpiece_176 tokenizer is not better than char_unigram_88. Also, its better value is not statistically significant comparing it to char_bpe_88 and char_bpe_176.\n",
    "\n",
    "We move on with getting 5 FCD values of the 2nd best tokenizer, i.e. char_unigram_88 and performing a two-sample t-test of char_unigram_88 and char_wordpiece_176.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c96da0a6378d19"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial 2nd best tokenizer char_unigram_88 has a mean of 0.2224 and a std.dev. of 0.0062\n",
      "\n",
      "Compare with sample of initial best tokenizer (char_wordpiece_176)\n",
      "Perform statistical test... (alternative: less)\n",
      "t-statistic: 0.829, p-value: 0.784, Reject H0 (p<0.05): False\n",
      "\n",
      "Compare with single FCD value of other tokenizers (descending by FCD)\n",
      "Perform statistical test... (alternative: less)\n",
      "\n",
      "char_wordpiece_88   : 0.2426\n",
      "t-statistic: -8.677, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "smarts_wordlevel_106: 0.2413\n",
      "t-statistic: -8.208, p-value: 0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_50   : 0.2393\n",
      "t-statistic: -7.462, p-value: 0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_44         : 0.2358\n",
      "t-statistic: -6.168, p-value: 0.002, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_44     : 0.2296\n",
      "t-statistic: -3.856, p-value: 0.009, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordlevel_38   : 0.2257\n",
      "t-statistic: -2.430, p-value: 0.036, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_176        : 0.2245\n",
      "t-statistic: -1.959, p-value: 0.061, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_88         : 0.2214\n",
      "t-statistic: -0.815, p-value: 0.230, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "guacamol_char_unigram_88 = Sample(\n",
    "    [\n",
    "        0.22215763836744884,\n",
    "        0.21625658903371914,\n",
    "        0.21653028109176375,\n",
    "        0.22646688628489642,\n",
    "        0.23060648678460893,\n",
    "    ]\n",
    ")\n",
    "\n",
    "guacamol_others: dict[str, float] = {\n",
    "    \"char_wordlevel_38\": 0.22573631123455584,\n",
    "    \"char_bpe_44\": 0.23582004914318588,\n",
    "    \"char_bpe_88\": 0.22138152297816305,\n",
    "    \"char_bpe_176\": 0.22446600524058624,\n",
    "    \"char_wordpiece_88\": 0.24258628303761043,\n",
    "    \"char_unigram_44\": 0.22958462926293066,\n",
    "    \"atom_wordlevel_50\": 0.23930838874149174,\n",
    "    \"smarts_wordlevel_106\": 0.24132050338971567,\n",
    "}\n",
    "\n",
    "print(\n",
    "    f\"Initial 2nd best tokenizer char_unigram_88 has a mean of {guacamol_char_unigram_88.mean:.4f} and a std.dev. of {guacamol_char_unigram_88.std:.4f}\"\n",
    ")\n",
    "print(\"\\nCompare with sample of initial best tokenizer (char_wordpiece_176)\")\n",
    "alternative = \"less\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "t_stat, p_val = two_sample_t_test(\n",
    "    guacamol_char_unigram_88, guacamol_char_wordpiece_176, alternative\n",
    ")\n",
    "print(\n",
    "    f\"t-statistic: {t_stat:.3f}, \"\n",
    "    f\"p-value: {p_val:.3f}, \"\n",
    "    f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nCompare with single FCD value of other tokenizers (descending by FCD)\")\n",
    "alternative = \"less\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "\n",
    "for tokenizer, fcd in sorted(guacamol_others.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"\\n{tokenizer:20s}: {fcd:.4f}\")\n",
    "    t_stat, p_val = one_sample_t_test(guacamol_char_wordpiece_176, fcd, alternative)\n",
    "    print(\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T11:26:21.360583366Z"
    }
   },
   "id": "225a73d0bbc30fa3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results for the 2nd “best” tokenizer\n",
    "\n",
    "After getting 5 FCD values of the 2nd best tokenizer, i.e. char_unigram_88 and performing a two-sample t-test of char_unigram_88 and char_wordpiece_176 we can not rejct H0, i.e. the two tokenizers are not statistically different. Therefore, we can conclude that we have a couple of tokenizers which seem to perform equally well.\n",
    "\n",
    "For the comparison with GuacaMol we select the model with a char_wordpiece_176 tokenizer with the lowest FCD value. The model directory is checkpoints/guacamol/tokenizers/char_wordpiece_176/2023-10-31_05-26-52_experiment.\n",
    "\n",
    "We generate 5 samples of 10,000 molecules, this time with a random seed for each sample. We then calculate the FCD for each sample and compare the 5 samples with the value of GuacaMol with a single-sample one-sided t-test."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66c8173859bc09a5"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tokenizer char_wordpiece_176 has a mean of 0.2252 and a std.dev. of 0.0073\n",
      "\n",
      "Compare with single FCD value of GuacaMol paper\n",
      "Perform statistical test... (alternative: less)\n",
      "t-statistic: -70.491, p-value: 0.000, Reject H0 (p<0.05): True\n"
     ]
    }
   ],
   "source": [
    "guacamol_char_wordpiece_176 = Sample(\n",
    "    [\n",
    "        0.2286251555231189,\n",
    "        0.23513992637839465,\n",
    "        0.21683749015160458,\n",
    "        0.2195920751401701,\n",
    "        0.22596241125135919,\n",
    "    ]\n",
    ")\n",
    "\n",
    "guacamol_original = 0.455\n",
    "\n",
    "print(\n",
    "    f\"Selected tokenizer char_wordpiece_176 has a mean of {guacamol_char_wordpiece_176.mean:.4f} and a std.dev. of {guacamol_char_wordpiece_176.std:.4f}\"\n",
    ")\n",
    "print(\"\\nCompare with single FCD value of GuacaMol paper\")\n",
    "alternative = \"less\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "\n",
    "t_stat, p_val = one_sample_t_test(\n",
    "    guacamol_char_wordpiece_176, guacamol_original, alternative\n",
    ")\n",
    "print(\n",
    "    f\"t-statistic: {t_stat:.3f}, \"\n",
    "    f\"p-value: {p_val:.3f}, \"\n",
    "    f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T11:26:21.408490516Z"
    }
   },
   "id": "2e97a46124536579"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other metrics of char_wordpiece_176:\n",
      "Other metrics of char_wordpiece_176:\n",
      "Metric: validity     Mean: 0.976   Std.dev. 0.002\n",
      "Metric: uniqueness   Mean: 0.999   Std.dev. 0.000\n",
      "Metric: novelty      Mean: 0.935   Std.dev. 0.002\n",
      "Metric: fcd_g_mols   Mean: 0.956   Std.dev. 0.001\n"
     ]
    }
   ],
   "source": [
    "# We then calculate mean and sigma of the other metrics\n",
    "\n",
    "validity = Sample(\n",
    "    [\n",
    "        0.9779,\n",
    "        0.9773,\n",
    "        0.9753,\n",
    "        0.9731,\n",
    "        0.976,\n",
    "    ]\n",
    ")\n",
    "\n",
    "uniqueness = Sample(\n",
    "    [\n",
    "        0.9989774005522037,\n",
    "        0.9995907090964903,\n",
    "        0.9994873372295704,\n",
    "        0.9993834138320831,\n",
    "        0.9995901639344262,\n",
    "    ]\n",
    ")\n",
    "\n",
    "novelty = Sample(\n",
    "    [\n",
    "        0.9317227966014945,\n",
    "        0.9375575801003173,\n",
    "        0.9344480919162905,\n",
    "        0.9374807197943444,\n",
    "        0.9353218532185322,\n",
    "    ]\n",
    ")\n",
    "\n",
    "fcd_g_mols = Sample(\n",
    "    [\n",
    "        0.955304605131761,\n",
    "        0.9540606975651333,\n",
    "        0.9575594241953026,\n",
    "        0.9570320337197208,\n",
    "        0.9558134869949808,\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Other metrics of char_wordpiece_176:\")\n",
    "metrics: dict[str, Sample] = {\n",
    "    \"validity\": validity,\n",
    "    \"uniqueness\": uniqueness,\n",
    "    \"novelty\": novelty,\n",
    "    \"fcd_g_mols\": fcd_g_mols,\n",
    "}\n",
    "\n",
    "print(\"Other metrics of char_wordpiece_176:\")\n",
    "for name, sample in metrics.items():\n",
    "    print(f\"Metric: {name:12s} Mean: {sample.mean:.3f}   Std.dev. {sample.std:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-13T11:26:21.408728450Z"
    }
   },
   "id": "8dc88b116213c6e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### USPTO50K/SMARTS\n",
    "\n",
    "#### The initial \"best\" tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bf7d52f64c2aa0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical tests for the \"best\" tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac201e60eb05b8a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: char_wordpiece_176   Single value: 557.000\n",
      "Tokenizer: char_wordpiece_94    Single value: 587.000\n",
      "Tokenizer: char_unigram_176     Single value: 616.000\n",
      "Tokenizer: char_bpe_176         Single value: 618.000\n",
      "Tokenizer: char_unigram_88      Single value: 621.000\n",
      "Tokenizer: char_bpe_88          Mean:         688.000   Std.dev. 0.001\n",
      "Tokenizer: char_bpe_47          Mean:         698.400   Std.dev. 0.001\n",
      "Tokenizer: char_wordlevel_47    Mean:         705.200   Std.dev. 0.001\n",
      "Tokenizer: atom_wordlevel_86    Mean:         730.800   Std.dev. 0.001\n",
      "Tokenizer: smarts_wordlevel_947 Mean:         735.200   Std.dev. 0.001\n"
     ]
    }
   ],
   "source": [
    "uspto50k_tokenizers: dict[str, Union[float, Sample]] = {\n",
    "    \"char_wordlevel_47\": Sample(\n",
    "        [\n",
    "            728.0,\n",
    "            726.0,\n",
    "            679.0,\n",
    "            699.0,\n",
    "            694.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_47\": Sample(\n",
    "        [\n",
    "            663.0,\n",
    "            686.0,\n",
    "            686.0,\n",
    "            693.0,\n",
    "            764.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            691.0,\n",
    "            631.0,\n",
    "            694.0,\n",
    "            694.0,\n",
    "            730.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": 618.0,\n",
    "    \"char_wordpiece_94\": 587.0,\n",
    "    \"char_wordpiece_176\": 557.0,\n",
    "    \"char_unigram_88\": 621.0,\n",
    "    \"char_unigram_176\": 616.0,\n",
    "    \"atom_wordlevel_86\": Sample(\n",
    "        [\n",
    "            775.0,\n",
    "            782.0,\n",
    "            670.0,\n",
    "            680.0,\n",
    "            747.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"smarts_wordlevel_947\": Sample(\n",
    "        [\n",
    "            746.0,\n",
    "            699.0,\n",
    "            771.0,\n",
    "            721.0,\n",
    "            739.0,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "print_basic_stats(uspto50k_tokenizers, reverse=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.484750175Z",
     "start_time": "2023-11-13T11:26:21.452252799Z"
    }
   },
   "id": "cb9db4b27af0c1db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "Based on this, the \"initial best\" tokenizer is smarts_wordlevel_947.\n",
    "\n",
    "Now we do a statistical test to see which tokenizers are statistically worse than smarts_wordlevel_947. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8033763e47ff9e5"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "def print_comparison(\n",
    "    baseline: Sample,\n",
    "    results: dict[str, Union[float, Sample]],\n",
    "    alternative: str = \"two_sided\",\n",
    "    reverse: bool = False,\n",
    "):\n",
    "    for name, comparison in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1]), reverse=reverse\n",
    "    ):\n",
    "        print(f\"\\n{name:20s}: {get_mean_value(comparison):.3f}\")\n",
    "        t_stat, p_val = t_test(baseline, comparison, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:.3f}, \"\n",
    "            f\"p-value: {p_val:.3f}, \"\n",
    "            f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T13:20:14.292628043Z",
     "start_time": "2023-11-13T13:20:14.239168887Z"
    }
   },
   "id": "4c9fed1c2765478"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compare with 'known' value(s) of other tokenizers (ascending by 'known')\n",
      "Perform statistical test... (alternative: greater)\n",
      "\n",
      "char_wordpiece_176  : 557.000\n",
      "t-statistic: 14.736, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordpiece_94   : 587.000\n",
      "t-statistic: 12.255, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_176    : 616.000\n",
      "t-statistic: 9.857, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_176        : 618.000\n",
      "t-statistic: 9.692, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_88     : 621.000\n",
      "t-statistic: 9.444, p-value: 0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_88         : 688.000\n",
      "t-statistic: 2.357, p-value: 0.023, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_47         : 698.400\n",
      "t-statistic: 1.753, p-value: 0.059, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordlevel_47   : 705.200\n",
      "t-statistic: 1.951, p-value: 0.043, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_86   : 730.800\n",
      "t-statistic: 0.166, p-value: 0.436, Reject H0 (p<0.05): False\n",
      "\n",
      "smarts_wordlevel_947: 735.200\n",
      "t-statistic: 0.000, p-value: 0.500, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCompare with 'known' value(s) of other tokenizers (ascending by 'known')\")\n",
    "alternative = \"greater\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "best_tokenizer = uspto50k_tokenizers[\"smarts_wordlevel_947\"]\n",
    "print_comparison(\n",
    "    best_tokenizer, uspto50k_tokenizers, alternative=\"greater\", reverse=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.485450101Z",
     "start_time": "2023-11-13T11:26:21.452623230Z"
    }
   },
   "id": "3e72f58d4d3cff8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "The \"initial best\" tokenizer smarts_wordlevel_947 is statistically better than most other tokenizers. The exceptions are char_bpe_47 and atom_wordlevel_86.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac74d96bad2e15f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.500430415Z",
     "start_time": "2023-11-13T11:26:21.459596190Z"
    }
   },
   "id": "d0bebbd7f3869b5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Archive\n",
    "\n",
    "The following cells have been used to check my understanding of the statistical tests and the scipy package."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbab5696061a5be1"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def compare_two_samples(s1: Sample, s2: Sample) -> tuple[float, float]:\n",
    "    t_stat, p_value = ttest_ind(s1.array, s2.array)  # just to check; fake!\n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "# Same calculation for both one and two sample t-tests\n",
    "def _calculate_p_value(t_statistic: float, df: int, alternative: str) -> float:\n",
    "    alternative = alternative.strip().lower()\n",
    "    if alternative == \"two_sided\":\n",
    "        p_value = (1 - t.cdf(abs(t_statistic), df)) * 2.0\n",
    "    elif alternative == \"less\":\n",
    "        p_value = t.cdf(t_statistic, df)\n",
    "    elif alternative == \"greater\":\n",
    "        p_value = 1 - t.cdf(t_statistic, df)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two_sided', 'greater' or 'smaller'\")\n",
    "\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-13T11:26:21.500691705Z",
     "start_time": "2023-11-13T11:26:21.500320827Z"
    }
   },
   "id": "27019476b7749854"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
