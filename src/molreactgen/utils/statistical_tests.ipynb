{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a6405014065f5e"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Final, Iterable, Optional, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, ttest_1samp, ttest_ind  # t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:15:12.213821144Z",
     "start_time": "2023-11-15T09:15:12.160407914Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "ALTERNATIVES = (\"two-sided\", \"less\", \"greater\")\n",
    "ALPHA = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.362310385Z",
     "start_time": "2023-11-15T09:10:29.360218327Z"
    }
   },
   "id": "444ab0135c733968"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sample:\n",
    "    values: Sequence[float]\n",
    "    array: np.ndarray = None\n",
    "    ddof: int = 1\n",
    "    mean: float = None\n",
    "    std: float = None\n",
    "    variance: float = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.array = np.array(self.values)\n",
    "        self.mean, self.std, self.var = self.get_basic_stats()\n",
    "\n",
    "    def get_basic_stats(self) -> tuple[float, float, float]:\n",
    "        mean = np.mean(self.array)\n",
    "        std = np.std(self.array, ddof=self.ddof)\n",
    "        var = np.var(self.array, ddof=self.ddof)\n",
    "        return mean, std, var\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Sample({self.values})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.array}\"\n",
    "\n",
    "\n",
    "MetricValue = Union[float, Sample]\n",
    "Metric = dict[str, MetricValue]\n",
    "TokenizerResults = dict[str, Metric]\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GuacaMolTokenizerResult:\n",
    "#     # tokenizer: str\n",
    "#     validity: Metric = None\n",
    "#     uniqueness: Metric = None\n",
    "#     novelty: Metric = None\n",
    "#     fcd: Metric = None\n",
    "#     # fcd_guacamol: Metric = None\n",
    "#\n",
    "#     @property\n",
    "#     def fcd_guacamol(self) -> Metric:\n",
    "#         if isinstance(self.fcd, float):\n",
    "#             return -5. * np.log(self.fcd)\n",
    "#         elif isinstance(self.fcd, Sample):\n",
    "#             values = [-5. * np.log(fcd) for fcd in self.fcd.values]\n",
    "#             return Sample(values)\n",
    "#         else:\n",
    "#             raise TypeError(\"Wrong type for fcd\")\n",
    "\n",
    "\n",
    "def build_tokenizer_results(\n",
    "    tokenizer_names: Sequence[str],\n",
    "    **kwargs,\n",
    ") -> TokenizerResults:\n",
    "    results: TokenizerResults = {}\n",
    "\n",
    "    for metric_name, metric in kwargs.items():\n",
    "        assert len(metric) == len(\n",
    "            tokenizer_names\n",
    "        ), f\"Incorrect number of metrics for {metric_name}\"\n",
    "        assert isinstance(metric, dict), f\"Incorrect metric {metric} in {metric_name}\"\n",
    "        for tokenizer_name, metric_value in metric.items():\n",
    "            assert (\n",
    "                tokenizer_name in tokenizer_names\n",
    "            ), f\"Incorrect tokenizer {tokenizer_name} in {metric_name}\"\n",
    "            assert isinstance(metric_value, float) or isinstance(\n",
    "                metric_value, Sample\n",
    "            ), f\"Incorrect metric value {metric_value} in {metric_name}\"\n",
    "\n",
    "    for tokenizer_name in tokenizer_names:\n",
    "        results[tokenizer_name] = {}\n",
    "\n",
    "    for metric_name, metric in kwargs.items():\n",
    "        for tokenizer_name, metric_value in metric.items():\n",
    "            results[tokenizer_name][metric_name] = metric_value\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_mean_value(metric_value: MetricValue) -> float:\n",
    "    # print(metric_value)\n",
    "    if isinstance(metric_value, float):\n",
    "        return metric_value\n",
    "    elif isinstance(metric_value, Sample):\n",
    "        return metric_value.mean\n",
    "    else:\n",
    "        pass\n",
    "        # raise TypeError(f\"metric_value must be either float or Sample, but has type {type(metric_value)}\")\n",
    "\n",
    "\n",
    "def print_basic_stats(\n",
    "    results: TokenizerResults, sort_by: str = \"fcd\", reverse: bool = False\n",
    "):\n",
    "    reverse_string = \"descending\" if reverse else \"ascending\"\n",
    "    print(\n",
    "        f\"Printing basic stats for {len(results)} tokenizers sorted by {sort_by.upper()} in {reverse_string.upper()} order...\"\n",
    "    )\n",
    "    # for tokenizer_name in sorted(\n",
    "    #         results, key=lambda x: get_mean_value(results[x][sort_by]), reverse=reverse\n",
    "    # ):\n",
    "    for tokenizer_name, metrics in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1][sort_by]), reverse=reverse\n",
    "    ):\n",
    "        print(f\"\\n*** {tokenizer_name.upper()} ***\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            if isinstance(metric_value, float):\n",
    "                print(f\"Metric: {metric_name:15s} Single value: {metric_value:.3f}\")\n",
    "            elif isinstance(metric_value, Sample):\n",
    "                print(\n",
    "                    f\"Metric: {metric_name:15s} Mean:         {metric_value.mean:.3f}   Std.dev. {metric_value.std:.3f}\"\n",
    "                )\n",
    "            else:\n",
    "                raise TypeError(\"metric must be a dict[str, dict[float|Sample]]\")\n",
    "\n",
    "\n",
    "def get_best_tokenizer(\n",
    "    results: TokenizerResults,\n",
    "    sort_by: str = \"fcd\",\n",
    "    reverse: bool = False,\n",
    "    with_sample: bool = True,\n",
    ") -> tuple[str, MetricValue]:\n",
    "    best_tokenizers = sorted(\n",
    "        results, key=lambda x: get_mean_value(results[x][sort_by]), reverse=reverse\n",
    "    )\n",
    "    if not with_sample:\n",
    "        best_tokenizer = best_tokenizers[-1]\n",
    "    else:\n",
    "        tokenizers_with_samples = get_tokenizers_with_sample(results, criterion=sort_by)\n",
    "        i = -1\n",
    "        while best_tokenizer := best_tokenizers[i]:\n",
    "            if best_tokenizer in tokenizers_with_samples:\n",
    "                break\n",
    "            else:\n",
    "                i -= 1\n",
    "\n",
    "    metric_value = results[best_tokenizer][sort_by]\n",
    "    return best_tokenizer, metric_value\n",
    "\n",
    "\n",
    "def get_tokenizers_with_sample(\n",
    "    results: TokenizerResults, criterion: str = \"fcd\"\n",
    ") -> list[str]:\n",
    "    result = []\n",
    "    tokenizers = results.keys()\n",
    "    for tokenizer in tokenizers:\n",
    "        metric_value = results[tokenizer][criterion]\n",
    "        if not isinstance(metric_value, float):  # (metric_value, Sample):\n",
    "            result.append(tokenizer)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:47:53.805377433Z",
     "start_time": "2023-11-15T12:47:53.766184212Z"
    }
   },
   "id": "2e1f4a4d72352cdf"
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_wordlevel_38\n",
      "0.22573631123455584\n",
      "char_unigram_44\n",
      "0.22958462926293066\n",
      "char_unigram_176\n",
      "0.2323379188153183\n",
      "char_wordpiece_88\n",
      "0.24258628303761043\n",
      "smarts_wordlevel_106\n",
      "0.24132050338971567\n",
      "char_bpe_44\n",
      "0.23582004914318588\n",
      "char_bpe_88\n",
      "[0.22660485 0.22836992 0.21916331 0.21487186 0.22354562]\n",
      "char_bpe_176\n",
      "[0.22267958 0.22500785 0.21600955 0.21789636 0.21858164]\n",
      "char_unigram_88\n",
      "[0.22215764 0.21625659 0.21653028 0.22646689 0.23060649]\n",
      "atom_wordlevel_50\n",
      "0.23930838874149174\n",
      "char_wordpiece_176\n",
      "[0.21138083 0.22424113 0.22342369 0.21396691 0.22290332]\n"
     ]
    },
    {
     "data": {
      "text/plain": "['char_bpe_88', 'char_bpe_176', 'char_unigram_88', 'char_wordpiece_176']"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokenizers_with_sample(guacamol_tokenizers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:46:26.606643072Z",
     "start_time": "2023-11-15T12:46:26.537940158Z"
    }
   },
   "id": "9ac72dad7d7d73ee"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_wordlevel_38\n",
      "0.22573631123455584\n",
      "char_unigram_44\n",
      "0.22958462926293066\n",
      "char_unigram_176\n",
      "0.2323379188153183\n",
      "char_wordpiece_88\n",
      "0.24258628303761043\n",
      "smarts_wordlevel_106\n",
      "0.24132050338971567\n",
      "char_bpe_44\n",
      "0.23582004914318588\n",
      "char_bpe_88\n",
      "[0.22660485 0.22836992 0.21916331 0.21487186 0.22354562]\n",
      "char_bpe_176\n",
      "[0.22267958 0.22500785 0.21600955 0.21789636 0.21858164]\n",
      "char_unigram_88\n",
      "[0.22215764 0.21625659 0.21653028 0.22646689 0.23060649]\n",
      "atom_wordlevel_50\n",
      "0.23930838874149174\n",
      "char_wordpiece_176\n",
      "[0.21138083 0.22424113 0.22342369 0.21396691 0.22290332]\n"
     ]
    }
   ],
   "source": [
    "t, x = get_best_tokenizer(guacamol_tokenizers, \"fcd\", reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:47:02.464024802Z",
     "start_time": "2023-11-15T12:47:02.390020950Z"
    }
   },
   "id": "aebca29117706e43"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "Sample([0.21138082989205031, 0.22424112983600253, 0.22342369151375863, 0.21396691344180852, 0.22290331624026294])"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:47:12.089279158Z",
     "start_time": "2023-11-15T12:47:12.071123112Z"
    }
   },
   "id": "ad668f71655e811"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The statistical tests"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be786b18c8ea5098"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def is_normally_distributed(sample: Sample, alpha: float = ALPHA):\n",
    "    _, p_value = shapiro(sample.array)\n",
    "    return p_value > alpha"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.393298285Z",
     "start_time": "2023-11-15T09:10:29.374844559Z"
    }
   },
   "id": "c0d4cdc77bbde306"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def one_sample_t_test(\n",
    "    sample: Sample, population_mean: float, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    # n = len(sample)\n",
    "    # df = n - 1\n",
    "    # t_statistic = (sample.mean - population_mean) / (sample.std / np.sqrt(n))\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    test_statistic, p_value = ttest_1samp(\n",
    "        sample.array, population_mean, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.400584599Z",
     "start_time": "2023-11-15T09:10:29.390083011Z"
    }
   },
   "id": "d2ce292f1d0d6014"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def two_sample_t_test(\n",
    "    sample: Sample, baseline: Sample, alternative: str = \"two_sided\"\n",
    ") -> tuple[float, float]:\n",
    "    # n1 = len(sample)\n",
    "    # n2 = len(baseline)\n",
    "    if sample.std > 2.0 * baseline.std or baseline.std > 2.0 * sample.std:\n",
    "        print(\"Warning: standard deviations differ by more than a factor of 2\")\n",
    "        print(\"Therefore, we do a Welch's t-test instead of a Student's t-test!\")\n",
    "        equal_var = False\n",
    "    else:\n",
    "        equal_var = True\n",
    "\n",
    "    # pooled_std = np.sqrt(((n1 - 1) * sample.std ** 2 + (n2 - 1) * baseline.std ** 2) / (n1 + n2 - 2))\n",
    "    # t_statistic = (sample.mean - baseline.mean) / (pooled_std * np.sqrt(1 / n1 + 1 / n2))\n",
    "    # df = n1 + n2 - 2\n",
    "    # p_value = _calculate_p_value(t_statistic, df, alternative)\n",
    "\n",
    "    if not is_normally_distributed(sample):\n",
    "        print(\"Warning: sample is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "    if not is_normally_distributed(baseline):\n",
    "        print(\"Warning: baseline is not normally distributed!\")\n",
    "        # print(\"Therefore, we do a Wilcoxon signed-rank test instead of a Student's t-test!\")\n",
    "        # test_statistic, p_value = wilcoxon(sample.array - population_mean, alternative=alternative)\n",
    "        # return test_statistic, p_value\n",
    "\n",
    "    test_statistic, p_value = ttest_ind(\n",
    "        sample.array, baseline.array, equal_var=equal_var, alternative=alternative\n",
    "    )\n",
    "    return test_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.446267767Z",
     "start_time": "2023-11-15T09:10:29.400496810Z"
    }
   },
   "id": "1eaaa267aedff3ba"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def interpret_p_value(\n",
    "    p_value: float,\n",
    "    # alternative: str,\n",
    "    alpha: float = ALPHA,\n",
    ") -> str:\n",
    "    if p_value < alpha:\n",
    "        return \"Reject H0\"\n",
    "    else:\n",
    "        return \"Cannot reject H0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.451178785Z",
     "start_time": "2023-11-15T09:10:29.448916666Z"
    }
   },
   "id": "46ab88ffb3d0f902"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def t_test(\n",
    "    sample: Sample, comparison: Union[float, Sample], alternative: str = \"two_sided\"\n",
    "):\n",
    "    if isinstance(comparison, float):\n",
    "        t_statistic, p_value = one_sample_t_test(sample, comparison, alternative)\n",
    "    elif isinstance(comparison, Sample):\n",
    "        t_statistic, p_value = two_sample_t_test(sample, comparison, alternative)\n",
    "    else:\n",
    "        raise ValueError(\"comparison must be a float or a Sample\")\n",
    "\n",
    "    return t_statistic, p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.489815933Z",
     "start_time": "2023-11-15T09:10:29.449409782Z"
    }
   },
   "id": "91c8db71576e9a1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example Usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279ffc159089ec11"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk test example\n",
      "Sample is normally distributed: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapiro-Wilk test example\")\n",
    "sample = Sample([2.5, 3.1, 2.8, 3.4, 2.9, 3.0, 3.3, 2.6, 3.2, 3.1])\n",
    "result = is_normally_distributed(sample)\n",
    "print(f\"Sample is normally distributed: {result}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.490088252Z",
     "start_time": "2023-11-15T09:10:29.489661267Z"
    }
   },
   "id": "fbf7881d94b693a0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample t-test example\n",
      "Sample: [10. 11. 12. 13. 14.])\n",
      "Population mean: 14.0\n",
      "Alternative: two-sided, t-statistic: -2.828, p-value: 0.047, Reject H0 (p<0.05): True\n",
      "Alternative: less     , t-statistic: -2.828, p-value: 0.024, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.828, p-value: 0.976, Reject H0 (p<0.05): False\n",
      "\n",
      "Two sample t-test example\n",
      "Sample: [10. 11. 12.])\n",
      "Baseline: [12. 13. 14.])\n",
      "Alternative: two-sided, t-statistic: -2.449, p-value: 0.070, Reject H0 (p<0.05): False\n",
      "Alternative: less     , t-statistic: -2.449, p-value: 0.035, Reject H0 (p<0.05): True\n",
      "Alternative: greater  , t-statistic: -2.449, p-value: 0.965, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "print(\"One sample t-test example\")\n",
    "sample = Sample([10.0, 11.0, 12.0, 13.0, 14.0])\n",
    "population_mean = 14.0\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Population mean: {population_mean}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = one_sample_t_test(sample, population_mean, alternative)\n",
    "    t_stat, p_val = t_test(sample, population_mean, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nTwo sample t-test example\")\n",
    "sample = Sample(np.array([10.0, 11.0, 12.0]))\n",
    "baseline = Sample(np.array([12.0, 13.0, 14.0]))\n",
    "print(f\"Sample: {sample}\")\n",
    "print(f\"Baseline: {baseline}\")\n",
    "for alternative in ALTERNATIVES:\n",
    "    # t_stat, p_val = two_sample_t_test(sample, baseline, alternative)\n",
    "    t_stat, p_val = t_test(sample, baseline, alternative)\n",
    "    print(\n",
    "        f\"Alternative: {alternative:9s}, \"\n",
    "        f\"t-statistic: {t_stat:.3f}, \"\n",
    "        f\"p-value: {p_val:.3f}, \"\n",
    "        f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T09:10:29.492553788Z",
     "start_time": "2023-11-15T09:10:29.490175957Z"
    }
   },
   "id": "84baad5a7392d8a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizer Experiments\n",
    "\n",
    "## Guacamol/SMILES\n",
    "\n",
    "### The initial \"best\" tokenizer\n",
    "\n",
    "This is based on a single run for each tokenizer.\n",
    "A single run consists of:\n",
    "\n",
    "- a training with fixed seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fb8d2d2b2f7ebb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical tests for the \"best\" tokenizer\n",
    "\n",
    "This is based on 5 runs for the \"best\" tokenizer.\n",
    "The 5 runs are:\n",
    "\n",
    "- a training with random seed\n",
    "- a generation with fixed seed\n",
    "- FCD as the metric\n",
    "\n",
    "At this point we have 5 FCD values for the \"best\" tokenizer.\n",
    "We then do a one-sample t-test and compare the sample with the \"next best\" tokenizer. If the current \"best\" tokenizer can be considered \"done\" and we declare the \"best\" tokenizer as the \"winner\". If not, we repeat the process with the \"next best\" tokenizer and perform a two-sample t-test. We repeat this process until we have a winner, or, we conculude that we have a couple of tokenizers which seem to perform equally well.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c7f589001ae7dc"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "GUACAMOL_TOKENIZERS: Final[set[str]] = {\n",
    "    \"char_wordlevel_38\",\n",
    "    \"char_bpe_44\",\n",
    "    \"char_bpe_88\",\n",
    "    \"char_bpe_176\",\n",
    "    \"char_wordpiece_88\",\n",
    "    \"char_wordpiece_176\",\n",
    "    \"char_unigram_44\",\n",
    "    \"char_unigram_88\",\n",
    "    \"char_unigram_176\",\n",
    "    \"atom_wordlevel_50\",\n",
    "    \"smarts_wordlevel_106\",\n",
    "}\n",
    "validity: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9822,\n",
    "    \"char_bpe_44\": 0.9826,\n",
    "    \"char_bpe_88\": Sample([0.9768, 0.9781, 0.9763, 0.9746, 0.9763]),\n",
    "    \"char_bpe_176\": Sample([0.9769, 0.9715, 0.9753, 0.9753, 0.9754]),\n",
    "    \"char_wordpiece_88\": 0.986,\n",
    "    \"char_wordpiece_176\": Sample([0.9779, 0.9769, 0.9759, 0.9757, 0.9753]),\n",
    "    \"char_unigram_44\": 0.9833,\n",
    "    \"char_unigram_88\": Sample([0.9759, 0.9767, 0.9738, 0.9781, 0.9774]),\n",
    "    \"char_unigram_176\": 0.9756,\n",
    "    \"atom_wordlevel_50\": 0.9805,\n",
    "    \"smarts_wordlevel_106\": 0.9796,\n",
    "}\n",
    "uniqueness: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9997963754836082,\n",
    "    \"char_bpe_44\": 0.9990840626908203,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.9987714987714987,\n",
    "            0.999386565790819,\n",
    "            0.9998975724674792,\n",
    "            0.9995895752103428,\n",
    "            0.9990781522073133,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.9992834476404955,\n",
    "            0.9997941327843541,\n",
    "            0.9988721419050548,\n",
    "            0.9989746744591408,\n",
    "            0.9992823457043264,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.9989858012170385,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.9986706207178648,\n",
    "            0.9992834476404955,\n",
    "            0.9995901219387232,\n",
    "            0.9994875474018653,\n",
    "            0.9993848046754845,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.9992881114614055,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.9990777743621273,\n",
    "            0.9993856864953414,\n",
    "            0.9995892380365579,\n",
    "            0.999591043860546,\n",
    "            0.9996930632289748,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.9996924969249692,\n",
    "    \"atom_wordlevel_50\": 0.9990821009688934,\n",
    "    \"smarts_wordlevel_106\": 0.9991833401388321,\n",
    "}\n",
    "novelty: Metric = {\n",
    "    \"char_wordlevel_38\": 0.9467413441955194,\n",
    "    \"char_bpe_44\": 0.9446877864928186,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.9407544075440755,\n",
    "            0.9358567774936062,\n",
    "            0.9337225978283139,\n",
    "            0.9397454321494559,\n",
    "            0.9347959811359442,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.9302397049784881,\n",
    "            0.9310202820961598,\n",
    "            0.9360500923834941,\n",
    "            0.9343118136097711,\n",
    "            0.9348517492561814,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.9361421319796954,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.9376407945934876,\n",
    "            0.933620159803319,\n",
    "            0.9340850845720143,\n",
    "            0.9366283839212469,\n",
    "            0.9352621319380322,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.9384286586606961,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.936,\n",
    "            0.941399446777994,\n",
    "            0.939284980480789,\n",
    "            0.9391428863659609,\n",
    "            0.9359328625524511,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.9366348815749,\n",
    "    \"atom_wordlevel_50\": 0.9432421396488363,\n",
    "    \"smarts_wordlevel_106\": 0.9406416019615856,\n",
    "}\n",
    "fcd: Metric = {\n",
    "    \"char_wordlevel_38\": 0.22573631123455584,\n",
    "    \"char_bpe_44\": 0.23582004914318588,\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            0.22660485100020367,\n",
    "            0.2283699198492286,\n",
    "            0.21916330554410024,\n",
    "            0.21487186290418947,\n",
    "            0.22354561820675656,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": Sample(\n",
    "        [\n",
    "            0.22267958128769294,\n",
    "            0.2250078470330834,\n",
    "            0.21600954762025992,\n",
    "            0.21789636102157317,\n",
    "            0.21858163757028137,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_wordpiece_88\": 0.24258628303761043,\n",
    "    \"char_wordpiece_176\": Sample(\n",
    "        [\n",
    "            0.21138082989205031,\n",
    "            0.22424112983600253,\n",
    "            0.22342369151375863,\n",
    "            0.21396691344180852,\n",
    "            0.22290331624026294,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_44\": 0.22958462926293066,\n",
    "    \"char_unigram_88\": Sample(\n",
    "        [\n",
    "            0.22215763836744884,\n",
    "            0.21625658903371914,\n",
    "            0.21653028109176375,\n",
    "            0.22646688628489642,\n",
    "            0.23060648678460893,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_unigram_176\": 0.2323379188153183,\n",
    "    \"atom_wordlevel_50\": 0.23930838874149174,\n",
    "    \"smarts_wordlevel_106\": 0.24132050338971567,\n",
    "}\n",
    "\n",
    "\n",
    "def get_fcd_guacamol(fcd: Metric) -> Metric:\n",
    "    fcd_guacamol: Metric = {}\n",
    "    for tokenizer_name, fcd_value in fcd.items():\n",
    "        if isinstance(fcd_value, float):\n",
    "            fcd_guacamol[tokenizer_name] = np.exp(-0.2 * fcd_value)\n",
    "        elif isinstance(fcd_value, Sample):\n",
    "            values = [np.exp(-0.2 * fcd) for fcd in fcd_value.values]\n",
    "            fcd_guacamol[tokenizer_name] = Sample(values)\n",
    "        else:\n",
    "            raise TypeError(\"Wrong type for fcd\")\n",
    "    return fcd_guacamol\n",
    "\n",
    "\n",
    "fcd_guacamol = get_fcd_guacamol(fcd)\n",
    "\n",
    "guacamol_tokenizers = build_tokenizer_results(\n",
    "    GUACAMOL_TOKENIZERS,\n",
    "    validity=validity,\n",
    "    uniqueness=uniqueness,\n",
    "    novelty=novelty,\n",
    "    fcd=fcd,\n",
    "    fcd_guacamol=fcd_guacamol,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:48:03.593035269Z",
     "start_time": "2023-11-15T12:48:03.520476598Z"
    }
   },
   "id": "9fe48796ddba5c55"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing basic stats for 11 tokenizers sorted by FCD in DESCENDING order...\n",
      "\n",
      "*** CHAR_WORDPIECE_88 ***\n",
      "Metric: validity        Single value: 0.986\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.936\n",
      "Metric: fcd             Single value: 0.243\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** SMARTS_WORDLEVEL_106 ***\n",
      "Metric: validity        Single value: 0.980\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.941\n",
      "Metric: fcd             Single value: 0.241\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** ATOM_WORDLEVEL_50 ***\n",
      "Metric: validity        Single value: 0.981\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.943\n",
      "Metric: fcd             Single value: 0.239\n",
      "Metric: fcd_guacamol    Single value: 0.953\n",
      "\n",
      "*** CHAR_BPE_44 ***\n",
      "Metric: validity        Single value: 0.983\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.945\n",
      "Metric: fcd             Single value: 0.236\n",
      "Metric: fcd_guacamol    Single value: 0.954\n",
      "\n",
      "*** CHAR_UNIGRAM_176 ***\n",
      "Metric: validity        Single value: 0.976\n",
      "Metric: uniqueness      Single value: 1.000\n",
      "Metric: novelty         Single value: 0.937\n",
      "Metric: fcd             Single value: 0.232\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_UNIGRAM_44 ***\n",
      "Metric: validity        Single value: 0.983\n",
      "Metric: uniqueness      Single value: 0.999\n",
      "Metric: novelty         Single value: 0.938\n",
      "Metric: fcd             Single value: 0.230\n",
      "Metric: fcd_guacamol    Single value: 0.955\n",
      "\n",
      "*** CHAR_WORDLEVEL_38 ***\n",
      "Metric: validity        Single value: 0.982\n",
      "Metric: uniqueness      Single value: 1.000\n",
      "Metric: novelty         Single value: 0.947\n",
      "Metric: fcd             Single value: 0.226\n",
      "Metric: fcd_guacamol    Single value: 0.956\n",
      "\n",
      "*** CHAR_BPE_88 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.001\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.937   Std.dev. 0.003\n",
      "Metric: fcd             Mean:         0.223   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.956   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_UNIGRAM_88 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.002\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.938   Std.dev. 0.002\n",
      "Metric: fcd             Mean:         0.222   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.956   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_BPE_176 ***\n",
      "Metric: validity        Mean:         0.975   Std.dev. 0.002\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.933   Std.dev. 0.003\n",
      "Metric: fcd             Mean:         0.220   Std.dev. 0.004\n",
      "Metric: fcd_guacamol    Mean:         0.957   Std.dev. 0.001\n",
      "\n",
      "*** CHAR_WORDPIECE_176 ***\n",
      "Metric: validity        Mean:         0.976   Std.dev. 0.001\n",
      "Metric: uniqueness      Mean:         0.999   Std.dev. 0.000\n",
      "Metric: novelty         Mean:         0.935   Std.dev. 0.002\n",
      "Metric: fcd             Mean:         0.219   Std.dev. 0.006\n",
      "Metric: fcd_guacamol    Mean:         0.957   Std.dev. 0.001\n"
     ]
    }
   ],
   "source": [
    "print_basic_stats(guacamol_tokenizers, sort_by=\"fcd\", reverse=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:43:24.439553856Z",
     "start_time": "2023-11-15T12:43:24.431584362Z"
    }
   },
   "id": "f7a7b5e2286e9f8a"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_wordpiece_176 [0.21138083 0.22424113 0.22342369 0.21396691 0.22290332]\n"
     ]
    }
   ],
   "source": [
    "best_tokenizer, value = get_best_tokenizer(\n",
    "    guacamol_tokenizers, sort_by=\"fcd\", reverse=True\n",
    ")\n",
    "print(best_tokenizer, value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:48:09.630364433Z",
     "start_time": "2023-11-15T12:48:09.618434746Z"
    }
   },
   "id": "7bdec98cc662559a"
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric VALIDITY with other tokenizers (ascending order\n",
      "Alternative: <built-in method upper of str object at 0x7fbda35a4fb0>\n",
      "Best tokenizer: char_bpe_88 with metric mean 0.976 and std.dev. 0.001\n",
      "\n",
      "char_bpe_176         with metric mean 0.975\n",
      "t-statistic:  1.454, p-value:  0.092, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_176     with metric mean 0.976\n",
      "t-statistic:  1.460, p-value:  0.109, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordpiece_176   with metric mean 0.976\n",
      "t-statistic:  0.109, p-value:  0.458, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88      with metric mean 0.976\n",
      "t-statistic:  0.043, p-value:  0.483, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_88          with metric mean 0.976\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n",
      "\n",
      "smarts_wordlevel_106 with metric mean 0.980\n",
      "t-statistic: -5.662, p-value:  0.998, Reject H0 (p<0.05): False\n",
      "\n",
      "atom_wordlevel_50    with metric mean 0.981\n",
      "t-statistic: -7.265, p-value:  0.999, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordlevel_38    with metric mean 0.982\n",
      "t-statistic: -10.292, p-value:  1.000, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_44          with metric mean 0.983\n",
      "t-statistic: -11.004, p-value:  1.000, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_44      with metric mean 0.983\n",
      "t-statistic: -12.251, p-value:  1.000, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordpiece_88    with metric mean 0.986\n",
      "t-statistic: -17.058, p-value:  1.000, Reject H0 (p<0.05): False\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric NOVELTY with other tokenizers (ascending order\n",
      "Alternative: <built-in method upper of str object at 0x7fbda35a4fb0>\n",
      "Best tokenizer: char_unigram_88 with metric mean 0.938 and std.dev. 0.002\n",
      "\n",
      "char_bpe_176         with metric mean 0.933\n",
      "t-statistic:  3.274, p-value:  0.006, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordpiece_176   with metric mean 0.935\n",
      "t-statistic:  2.241, p-value:  0.028, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordpiece_88    with metric mean 0.936\n",
      "t-statistic:  2.099, p-value:  0.052, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_176     with metric mean 0.937\n",
      "t-statistic:  1.631, p-value:  0.089, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_88          with metric mean 0.937\n",
      "t-statistic:  0.790, p-value:  0.226, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88      with metric mean 0.938\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_44      with metric mean 0.938\n",
      "t-statistic: -0.073, p-value:  0.527, Reject H0 (p<0.05): False\n",
      "\n",
      "smarts_wordlevel_106 with metric mean 0.941\n",
      "t-statistic: -2.175, p-value:  0.952, Reject H0 (p<0.05): False\n",
      "\n",
      "atom_wordlevel_50    with metric mean 0.943\n",
      "t-statistic: -4.645, p-value:  0.995, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_44          with metric mean 0.945\n",
      "t-statistic: -6.018, p-value:  0.998, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordlevel_38    with metric mean 0.947\n",
      "t-statistic: -7.969, p-value:  0.999, Reject H0 (p<0.05): False\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric UNIQUENESS with other tokenizers (ascending order\n",
      "Alternative: <built-in method upper of str object at 0x7fbda35a4fb0>\n",
      "Best tokenizer: char_unigram_88 with metric mean 0.999 and std.dev. 0.000\n",
      "\n",
      "char_wordpiece_88    with metric mean 0.999\n",
      "t-statistic:  4.400, p-value:  0.006, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_50    with metric mean 0.999\n",
      "t-statistic:  3.520, p-value:  0.012, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_44          with metric mean 0.999\n",
      "t-statistic:  3.502, p-value:  0.012, Reject H0 (p<0.05): True\n",
      "\n",
      "smarts_wordlevel_106 with metric mean 0.999\n",
      "t-statistic:  2.595, p-value:  0.030, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_176         with metric mean 0.999\n",
      "t-statistic:  1.162, p-value:  0.139, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordpiece_176   with metric mean 0.999\n",
      "t-statistic:  0.944, p-value:  0.187, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_44      with metric mean 0.999\n",
      "t-statistic:  1.638, p-value:  0.088, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_88          with metric mean 0.999\n",
      "t-statistic:  0.547, p-value:  0.300, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88      with metric mean 0.999\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_176     with metric mean 1.000\n",
      "t-statistic: -2.057, p-value:  0.946, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordlevel_38    with metric mean 1.000\n",
      "t-statistic: -3.006, p-value:  0.980, Reject H0 (p<0.05): False\n",
      "\n",
      "*** Perform statistical tests ***\n",
      "Compare metric FCD with other tokenizers (descending order\n",
      "Alternative: <built-in method upper of str object at 0x7fbda29abef0>\n",
      "Best tokenizer: char_wordpiece_176 with metric mean 0.219 and std.dev. 0.006\n",
      "\n",
      "char_wordpiece_88    with metric mean 0.243\n",
      "t-statistic: -8.677, p-value:  0.000, Reject H0 (p<0.05): True\n",
      "\n",
      "smarts_wordlevel_106 with metric mean 0.241\n",
      "t-statistic: -8.208, p-value:  0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "atom_wordlevel_50    with metric mean 0.239\n",
      "t-statistic: -7.462, p-value:  0.001, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_44          with metric mean 0.236\n",
      "t-statistic: -6.168, p-value:  0.002, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_176     with metric mean 0.232\n",
      "t-statistic: -4.877, p-value:  0.004, Reject H0 (p<0.05): True\n",
      "\n",
      "char_unigram_44      with metric mean 0.230\n",
      "t-statistic: -3.856, p-value:  0.009, Reject H0 (p<0.05): True\n",
      "\n",
      "char_wordlevel_38    with metric mean 0.226\n",
      "t-statistic: -2.430, p-value:  0.036, Reject H0 (p<0.05): True\n",
      "\n",
      "char_bpe_88          with metric mean 0.223\n",
      "t-statistic: -0.910, p-value:  0.195, Reject H0 (p<0.05): False\n",
      "\n",
      "char_unigram_88      with metric mean 0.222\n",
      "t-statistic: -0.829, p-value:  0.216, Reject H0 (p<0.05): False\n",
      "\n",
      "char_bpe_176         with metric mean 0.220\n",
      "t-statistic: -0.269, p-value:  0.397, Reject H0 (p<0.05): False\n",
      "\n",
      "char_wordpiece_176   with metric mean 0.219\n",
      "t-statistic:  0.000, p-value:  0.500, Reject H0 (p<0.05): False\n"
     ]
    }
   ],
   "source": [
    "def print_comparison(\n",
    "    tokenizer_name: str,\n",
    "    # tokenizer_sample: Sample,\n",
    "    results: TokenizerResults,\n",
    "    criterion: str = \"fcd\",\n",
    "    alternative: str = \"two_sided\",\n",
    "    reverse: bool = False,\n",
    "):\n",
    "    reverse_string = \"descending\" if reverse else \"ascending\"\n",
    "    print(\"\\n*** Perform statistical tests ***\")\n",
    "    print(\n",
    "        f\"Compare metric {criterion.upper()} with other tokenizers (in {reverse_string} order)\"\n",
    "    )\n",
    "    print(f\"Alternative: {alternative.upper()}\")\n",
    "    tokenizer_sample = results[tokenizer_name][criterion]\n",
    "    print(\n",
    "        f\"Best tokenizer: {tokenizer_name} with metric mean {tokenizer_sample.mean:.3f} and std.dev. {tokenizer_sample.std:.3f}\"\n",
    "    )\n",
    "    for tokenizer, metrics in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1][criterion]), reverse=reverse\n",
    "    ):\n",
    "        metric = metrics[criterion]\n",
    "        print(f\"\\n{tokenizer:20s} with metric mean {get_mean_value(metric):.3f}\")\n",
    "        t_stat, p_val = t_test(tokenizer_sample, metric, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:6.3f}, \"\n",
    "            f\"p-value: {p_val:6.3f}, \"\n",
    "            f\"Reject H0 (p<{ALPHA}): {p_val < ALPHA}\"\n",
    "        )\n",
    "\n",
    "\n",
    "criteria = {\"validity\", \"uniqueness\", \"novelty\", \"fcd\"}\n",
    "for criterion in criteria:\n",
    "    reverse = True if criterion == \"fcd\" else False\n",
    "    alternative = \"less\" if criterion == \"fcd\" else \"greater\"\n",
    "    best_tokenizer, _ = get_best_tokenizer(\n",
    "        guacamol_tokenizers, sort_by=criterion, reverse=reverse\n",
    "    )\n",
    "    print_comparison(\n",
    "        best_tokenizer,\n",
    "        guacamol_tokenizers,\n",
    "        criterion=criterion,\n",
    "        alternative=alternative,\n",
    "        reverse=reverse,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-15T12:56:34.660861134Z",
     "start_time": "2023-11-15T12:56:34.579227338Z"
    }
   },
   "id": "be83e163ef5e532b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results for the initial \"best\" tokenizer\n",
    "\n",
    "The initial \"best\" tokenizer is char_wordpiece_176. After 5 runs and 5 FCD values we see, that the mean of those 5 values is worse than the single char_unigram_88 value. Therefore, we can expect that the char_wordpiece_176 tokenizer is not better than char_unigram_88. Also, its better value is not statistically significant comparing it to char_bpe_88 and char_bpe_176.\n",
    "\n",
    "We move on with getting 5 FCD values of the 2nd best tokenizer, i.e. char_unigram_88 and performing a two-sample t-test of char_unigram_88 and char_wordpiece_176.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c96da0a6378d19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results for the 2nd “best” tokenizer\n",
    "\n",
    "After getting 5 FCD values of the 2nd best tokenizer, i.e. char_unigram_88 and performing a two-sample t-test of char_unigram_88 and char_wordpiece_176 we can not rejct H0, i.e. the two tokenizers are not statistically different. Therefore, we can conclude that we have a couple of tokenizers which seem to perform equally well.\n",
    "\n",
    "For the comparison with GuacaMol we select the model with a char_wordpiece_176 tokenizer with the lowest FCD value. The model directory is checkpoints/guacamol/tokenizers/char_wordpiece_176/2023-10-31_05-26-52_experiment.\n",
    "\n",
    "We generate 5 samples of 10,000 molecules, this time with a random seed for each sample. We then calculate the FCD for each sample and compare the 5 samples with the value of GuacaMol with a single-sample one-sided t-test."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66c8173859bc09a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "guacamol_char_wordpiece_176 = Sample(\n",
    "    [\n",
    "        0.2286251555231189,\n",
    "        0.23513992637839465,\n",
    "        0.21683749015160458,\n",
    "        0.2195920751401701,\n",
    "        0.22596241125135919,\n",
    "    ]\n",
    ")\n",
    "\n",
    "guacamol_original = 0.455\n",
    "\n",
    "print(\n",
    "    f\"Selected tokenizer char_wordpiece_176 has a mean of {guacamol_char_wordpiece_176.mean:.4f} and a std.dev. of {guacamol_char_wordpiece_176.std:.4f}\"\n",
    ")\n",
    "print(\"\\nCompare with single FCD value of GuacaMol paper\")\n",
    "alternative = \"less\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "\n",
    "t_stat, p_val = one_sample_t_test(\n",
    "    guacamol_char_wordpiece_176, guacamol_original, alternative\n",
    ")\n",
    "print(\n",
    "    f\"t-statistic: {t_stat:.3f}, \"\n",
    "    f\"p-value: {p_val:.3f}, \"\n",
    "    f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.686860732Z"
    }
   },
   "id": "2e97a46124536579"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### USPTO50K/SMARTS\n",
    "\n",
    "#### The initial \"best\" tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6bf7d52f64c2aa0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Statistical tests for the \"best\" tokenizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac201e60eb05b8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "uspto50k_tokenizers: dict[str, Union[float, Sample]] = {\n",
    "    \"char_wordlevel_47\": Sample(\n",
    "        [\n",
    "            728.0,\n",
    "            726.0,\n",
    "            679.0,\n",
    "            699.0,\n",
    "            694.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_47\": Sample(\n",
    "        [\n",
    "            663.0,\n",
    "            686.0,\n",
    "            686.0,\n",
    "            693.0,\n",
    "            764.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_88\": Sample(\n",
    "        [\n",
    "            691.0,\n",
    "            631.0,\n",
    "            694.0,\n",
    "            694.0,\n",
    "            730.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"char_bpe_176\": 618.0,\n",
    "    \"char_wordpiece_94\": 587.0,\n",
    "    \"char_wordpiece_176\": 557.0,\n",
    "    \"char_unigram_88\": 621.0,\n",
    "    \"char_unigram_176\": 616.0,\n",
    "    \"atom_wordlevel_86\": Sample(\n",
    "        [\n",
    "            775.0,\n",
    "            782.0,\n",
    "            670.0,\n",
    "            680.0,\n",
    "            747.0,\n",
    "        ]\n",
    "    ),\n",
    "    \"smarts_wordlevel_947\": Sample(\n",
    "        [\n",
    "            746.0,\n",
    "            699.0,\n",
    "            771.0,\n",
    "            721.0,\n",
    "            739.0,\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "print_basic_stats(uspto50k_tokenizers, reverse=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.690603926Z"
    }
   },
   "id": "cb9db4b27af0c1db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "Based on this, the \"initial best\" tokenizer is smarts_wordlevel_947.\n",
    "\n",
    "Now we do a statistical test to see which tokenizers are statistically worse than smarts_wordlevel_947. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8033763e47ff9e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def xx_print_comparison(\n",
    "    baseline: Sample,\n",
    "    results: dict[str, Union[float, Sample]],\n",
    "    alternative: str = \"two_sided\",\n",
    "    reverse: bool = False,\n",
    "):\n",
    "    for name, comparison in sorted(\n",
    "        results.items(), key=lambda x: get_mean_value(x[1]), reverse=reverse\n",
    "    ):\n",
    "        print(f\"\\n{name:20s}: {get_mean_value(comparison):.3f}\")\n",
    "        t_stat, p_val = t_test(baseline, comparison, alternative)\n",
    "        print(\n",
    "            f\"t-statistic: {t_stat:.3f}, \"\n",
    "            f\"p-value: {p_val:.3f}, \"\n",
    "            f\"Reject H0 (p<0.05): {p_val < 0.05}\"\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.692097855Z"
    }
   },
   "id": "4c9fed1c2765478"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\nCompare with 'known' value(s) of other tokenizers (ascending by 'known')\")\n",
    "alternative = \"greater\"\n",
    "print(f\"Perform statistical test... (alternative: {alternative})\")\n",
    "best_tokenizer = uspto50k_tokenizers[\"smarts_wordlevel_947\"]\n",
    "print_comparison(\n",
    "    best_tokenizer, uspto50k_tokenizers, alternative=\"greater\", reverse=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.710434972Z"
    }
   },
   "id": "3e72f58d4d3cff8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpretation\n",
    "\n",
    "The \"initial best\" tokenizer smarts_wordlevel_947 is statistically better than most other tokenizers. The exceptions are char_bpe_47 and atom_wordlevel_86.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac74d96bad2e15f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.710558794Z"
    }
   },
   "id": "d0bebbd7f3869b5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Archive\n",
    "\n",
    "The following cells have been used to check my understanding of the statistical tests and the scipy package."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbab5696061a5be1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_two_samples(s1: Sample, s2: Sample) -> tuple[float, float]:\n",
    "    t_stat, p_value = ttest_ind(s1.array, s2.array)  # just to check; fake!\n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "# Same calculation for both one and two sample t-tests\n",
    "def _calculate_p_value(t_statistic: float, df: int, alternative: str) -> float:\n",
    "    alternative = alternative.strip().lower()\n",
    "    if alternative == \"two_sided\":\n",
    "        p_value = (1 - t.cdf(abs(t_statistic), df)) * 2.0\n",
    "    elif alternative == \"less\":\n",
    "        p_value = t.cdf(t_statistic, df)\n",
    "    elif alternative == \"greater\":\n",
    "        p_value = 1 - t.cdf(t_statistic, df)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two_sided', 'greater' or 'smaller'\")\n",
    "\n",
    "    return p_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-15T09:10:29.710631460Z"
    }
   },
   "id": "27019476b7749854"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
